{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefect is a powerful **workflow orchestration** library for managing and automating data pipelines. It allows you to define, run, and monitor data workflows with ease, while handling common challenges like retries, scheduling, and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install prefect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "Prefect uses the concept of \"flows\" and \"tasks.\"\n",
    "\n",
    "- **Task**: An individual operation or step in a pipeline.\n",
    "- **Flow**: A collection of tasks that define the overall pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "Tasks are defined using the **@task** decorator. A task can be a function that performs a data transformation or other operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import task\n",
    "\n",
    "@task\n",
    "def extract_data():\n",
    "    # Simulate extracting data\n",
    "    return \"data\"\n",
    "\n",
    "@task\n",
    "def transform_data(data):\n",
    "    # Simulate transforming data\n",
    "    return f\"transformed {data}\"\n",
    "\n",
    "@task\n",
    "def load_data(data):\n",
    "    # Simulate loading data\n",
    "    print(f\"Loaded: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Flow class** holds the tasks. You can define dependencies between tasks by chaining them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import Flow\n",
    "from prefect import Parameter\n",
    "\n",
    "with Flow(\"ETL Pipeline\") as flow:\n",
    "    data = Parameter(\"data\")\n",
    "    \n",
    "    raw_data = extract_data(data)\n",
    "    transformed_data = transform_data(raw_data)\n",
    "    load_data(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "- **with Flow(\"ETL Pipeline\") as flow**: This creates a flow named \"ETL Pipeline\". All tasks defined inside this with block are part of the flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a flow locally, use the **flow.run()** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = flow.run(parameters={\n",
    "    \"data\": [\"alpha\", \"beta\", \"gamma\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing and Loading Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefect supports the storage of flows so that you can easily load and run them later. You can store flows on various platforms like **GitHub, S3, and Prefect Cloud**m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Flows Locally\n",
    "To store a flow locally, use the **Local storage class**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect.storage import Local\n",
    "\n",
    "# Define local storage and assign it to the flow\n",
    "flow.storage = Local(directory=\"flows/\")  # Save the flow to the \"flows/\" directory\n",
    "\n",
    "# Save the flow\n",
    "flow.save(\"my_local_flow.prefect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Flows\n",
    "Once your flow is stored, you can load it from storage. If you're using Prefect Cloud, itâ€™s automatically registered.\n",
    "\n",
    "- To load from a local file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import Flow\n",
    "\n",
    "# Load the flow\n",
    "loaded_flow = Flow.load(\"flows/my_local_flow.prefect\")\n",
    "\n",
    "# Run the loaded flow\n",
    "state = loaded_flow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduling and Monitoring Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefect provides easy **scheduling options** to **automate flow runs**. You can define schedules using the **CronSchedule** or **IntervalSchedule** classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling with Cron\n",
    "To run a flow periodically with a cron schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect.schedules import CronSchedule\n",
    "\n",
    "schedule = CronSchedule(\"0 0 * * *\")  # Every day at midnight\n",
    "\n",
    "with Flow(\"ETL Pipeline\", schedule=schedule) as flow:\n",
    "    raw_data = extract_data()\n",
    "    transformed_data = transform_data(raw_data)\n",
    "    load_data(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "- **CronSchedule(\"0 0 \\* \\* \\*\")** schedules the flow to run at midnight every day.\n",
    "- The **schedule** parameter is passed to the flow to automate the execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling with Interval\n",
    "For intervals, use the IntervalSchedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from prefect.schedules import IntervalSchedule\n",
    "\n",
    "schedule = IntervalSchedule(interval=timedelta(hours=1))  # Run every hour\n",
    "\n",
    "with Flow(\"ETL Pipeline\", schedule=schedule) as flow:\n",
    "    raw_data = extract_data()\n",
    "    transformed_data = transform_data(raw_data)\n",
    "    load_data(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "- **@task(max_retries=3, retry_delay=timedelta(seconds=10))** specifies that the task will be retried up to 3 times with a 10-second delay between retries if it fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Failures and Retries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefect allows **retry logic for tasks**. You can define retries with a maximum number of attempts and a delay between retries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import task\n",
    "from prefect.tasks.control_flow import fail\n",
    "\n",
    "@task(max_retries=3, retry_delay=timedelta(seconds=10))\n",
    "def risky_task():\n",
    "    # Simulate a task that might fail\n",
    "    print(\"Running risky task...\")\n",
    "    if random.random() < 0.5:\n",
    "        raise Exception(\"Task failed!\")\n",
    "    return \"Success\"\n",
    "\n",
    "with Flow(\"ETL Pipeline with Retry\") as flow:\n",
    "    result = risky_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prefect Executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefect supports multiple execution environments through executors. The most common ones are the **LocalExecutor** and **DaskExecutor** for parallel execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LocalExecutor (Default)\n",
    "By default, Prefect runs tasks sequentially using the LocalExecutor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DaskExecutor for Parallelism\n",
    "To use Dask for parallel execution, install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install prefect[extras] dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, use the **DaskExecuter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect.executors import DaskExecutor\n",
    "from prefect import Flow\n",
    "\n",
    "with Flow(\"Parallel ETL\", executor=DaskExecutor()) as flow:\n",
    "    data1 = extract_data()\n",
    "    data2 = extract_data()\n",
    "    transformed_data1 = transform_data(data1)\n",
    "    transformed_data2 = transform_data(data2)\n",
    "    load_data(transformed_data1)\n",
    "    load_data(transformed_data2)\n",
    "\n",
    "flow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For production, Prefect supports deployment options to cloud services like Kubernetes, Docker, and AWS Batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect.storage import Docker\n",
    "\n",
    "flow.storage = Docker(registry_url=\"your-docker-repo\", image_name=\"prefect-pipeline\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
