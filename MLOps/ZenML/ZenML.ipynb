{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ZenML** is an open-source MLOps library designed to manage the **end-to-end** lifecycle of machine learning (**ML**) pipelines. In this tutorial, we'll walk through how ZenML can be used in a typical data science project to manage the following steps:\n",
    "\n",
    "- Loading Data\n",
    "- Processing Data\n",
    "- Exploring Models\n",
    "- Fine-tuning Models\n",
    "- Deployment\n",
    "\n",
    "We will use ZenML to structure the pipeline and automate tasks like versioning, reproducibility, and monitoring, which will be beneficial for any ML project, including customer churn prediction (ChurnDetect), fraud detection, or any other ML use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install zenml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "### Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pipeline**:\n",
    "A pipeline is a sequence of steps that define the end-to-end workflow of your machine learning process. Each step performs a specific task like data cleaning, transformation, model training, etc.\n",
    "\n",
    "- **Step**:\n",
    "A step is a single operation or transformation in the pipeline. Steps can be anything from data preprocessing, feature engineering, training a model, to model evaluation.\n",
    "\n",
    "- **Artifact**:\n",
    "Artifacts are data objects passed between steps in the pipeline. They can be datasets, models, or any data produced or consumed by steps. Artifacts help ZenML track data flow between steps.\n",
    "\n",
    "- **DataArtifact**:\n",
    "A specific type of artifact that contains data (like a dataset). It is passed between the steps of the pipeline and can be saved or loaded from various sources.\n",
    "\n",
    "- **Component**:\n",
    "A component is a reusable piece of functionality that can be used in multiple steps. Components encapsulate code, such as data loaders, model trainers, or evaluators, and make it easier to reuse across pipelines.\n",
    "\n",
    "- **Metadata Store**:\n",
    "The metadata store tracks the state of your pipelines, steps, and artifacts. It stores the parameters, metrics, and outputs of each step, enabling reproducibility.\n",
    "\n",
    "- **Run**:\n",
    "A run is an execution of the pipeline. Each time you execute a pipeline, ZenML records the inputs, outputs, and metadata of the run.\n",
    "\n",
    "- **Experiment**:\n",
    "An experiment is a group of related runs. It helps in organizing different configurations or variations of a pipeline that you want to compare.\n",
    "\n",
    "- **Context**:\n",
    "Context refers to the environment or session in which a pipeline or step is executed. This includes the configuration, dependencies, and available resources.\n",
    "\n",
    "- **Artifact Store**:\n",
    "An artifact store is where artifacts are saved. It can be a cloud storage, local file system, or any other storage backend.\n",
    "\n",
    "- **Orchestrator**:\n",
    "An orchestrator manages the execution of ZenML pipelines. It helps execute the steps across distributed systems, and ZenML supports different orchestrators like Kubeflow, Airflow, and others.\n",
    "\n",
    "- **Visualizer**:\n",
    "Visualizer is a component used to visualize the results or outcomes of pipeline runs, including metrics, performance graphs, and other visualizations.\n",
    "\n",
    "- **Flavor**:\n",
    "A flavor is a version or a configuration of a component. For example, a model might have different flavors depending on the framework (like TensorFlow, PyTorch) used.\n",
    "\n",
    "- **Registry**:\n",
    "A registry stores different versions of components and models. It allows you to version and store your models or data transformations for reuse.\n",
    "\n",
    "- **Custom Step**:\n",
    "A custom step is a user-defined step that you create to perform a specific operation in your pipeline that isn't already available in ZenML.\n",
    "- **Versioning**:\n",
    "Versioning refers to tracking changes in models, steps, or data. ZenML tracks versions of artifacts, pipeline runs, and components to ensure reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZenML makes it easy to handle data loading by integrating with **various data sources**, ensuring that datasets are **versioned** and can be reused in **future experiments**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create a ZenML Artifact Store\n",
    "ZenML stores datasets and models as **artifacts**, and you need an artifact store to save these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.core import store\n",
    "from zenml.artifacts import DataArtifact\n",
    "\n",
    "# Initialize artifact store (local or cloud-based)\n",
    "artifact_store = store.get_artifact_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "### 1.2 Define a Data Loading Step\n",
    "You can create a custom data loading function as part of your ZenML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.steps import step\n",
    "\n",
    "@step\n",
    "def load_data() -> DataArtifact:\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv('customer_churn_data.csv')\n",
    "    return data  # Return the loaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Run the Step\n",
    "Load the data and track it as an artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading and storing the data\n",
    "data_step = load_data()\n",
    "data = data_step()\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits:\n",
    "- Data is versioned automatically by ZenML.\n",
    "- You can track different versions of the dataset used in your experiments, making it easy to go back to previous versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is loaded, it's time to preprocess it. This includes **cleaning** the data, **handling missing values**, **encoding categorical** variables, and **feature scaling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define Data Preprocessing Step:\n",
    "You can create a reusable data preprocessing function that will be part of your ZenML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.steps import step\n",
    "from zenml.pipelines import pipeline\n",
    "from zenml.artifacts import DataArtifact\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "@step\n",
    "def clean_data(data: DataArtifact) -> DataArtifact:\n",
    "    \"\"\"Remove missing values from the dataset.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    data = data.dropna()\n",
    "    return data\n",
    "\n",
    "@step\n",
    "def encode_categorical_features(data: DataArtifact) -> DataArtifact:\n",
    "    \"\"\"Encode categorical columns in the dataset.\"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    data['gender'] = encoder.fit_transform(data['gender'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "@step\n",
    "def feature_engineering(data: DataArtifact) -> DataArtifact:\n",
    "    \"\"\"Feature engineering for the dataset (creating new features if needed).\"\"\"\n",
    "    # Example: Adding a new feature based on existing ones (e.g., tenure * monthly_charges)\n",
    "    data['tenure_monthly_charge'] = data['tenure'] * data['monthly_charges']\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "@step\n",
    "def scale_data(data: DataArtifact) -> DataArtifact:\n",
    "    \"\"\"Scale numeric features in the dataset.\"\"\"\n",
    "    import pandas as pd\n",
    "    scaler = StandardScaler()\n",
    "    data[['tenure', 'monthly_charges', 'total_charges']] = scaler.fit_transform(\n",
    "        data[['tenure', 'monthly_charges', 'total_charges']]\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "@pipeline\n",
    "def churn_pipeline(raw_data: DataArtifact) -> DataArtifact:\n",
    "    cleaned_data = clean_data(data=raw_data)\n",
    "    encoded_data = encode_categorical_features(data=cleaned_data)\n",
    "    engineered_data = feature_engineering(data=encoded_data)\n",
    "    scaled_data = scale_data(data=engineered_data)\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Run Preprocessing Step:\n",
    "You can chain the data preprocessing step after the data loading step in your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "pipeline_instance = churn_pipeline()\n",
    "pipeline_instance.run()\n",
    "\n",
    "# Access the final output (scaled data)\n",
    "final_data = pipeline_instance.output\n",
    "print(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits:\n",
    "- Ensures that preprocessing steps are reproducible.\n",
    "- Allows easy adjustments and fine-tuning of preprocessing techniques during different experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploring Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define a Model Training Step:\n",
    "Create a reusable step for training models. This allows you to experiment with different models easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.steps import step\n",
    "from zenml.artifacts import DataArtifact, ModelArtifact\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "@step\n",
    "def train_model(data: DataArtifact) -> ModelArtifact:\n",
    "    # Split data into features and target\n",
    "    X = data.drop('churn', axis=1)\n",
    "    y = data['churn']\n",
    "    \n",
    "    # Train a Random Forest model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    predictions = model.predict(X)\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    print(f'Model accuracy: {accuracy}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring different models involves experimenting with various **machine learning algorithms** to find the best one for your task. ZenML allows you to organize and **track the different models** you test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Run Model Exploration Step:\n",
    "You can define multiple steps to explore different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = train_model(data=processed_data_result)\n",
    "model_result = random_forest_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits:\n",
    "- Keeps track of multiple models and their results, allowing easy comparisons.\n",
    "- Allows experimentation with various algorithms in a clean and organized way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fine-Tuning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define a Hyperparameter Tuning Step:\n",
    "You can create a tuning step using Optuna for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from zenml.steps import step\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "@step\n",
    "def tune_model(data: DataArtifact) -> ModelArtifact:\n",
    "    def objective(trial):\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 1, 20)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "        \n",
    "        # Train and evaluate model\n",
    "        X = data.drop('churn', axis=1)\n",
    "        y = data['churn']\n",
    "        model.fit(X, y)\n",
    "        score = model.score(X, y)\n",
    "        return score\n",
    "\n",
    "    # Start the hyperparameter search\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=10)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(f'Best hyperparameters: {best_params}')\n",
    "\n",
    "    # Train the best model with optimal parameters\n",
    "    best_model = RandomForestClassifier(**best_params)\n",
    "    best_model.fit(X, y)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Run Fine-Tuning Step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = tune_model(data=processed_data_result)\n",
    "tuned_model_result = tuned_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits:\n",
    "- Automates the hyperparameter tuning process.\n",
    "- Ensures that the model fine-tuning process is well-documented and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the best-performing model, ZenML can help you deploy it to a production environment. You can automate the deployment process as part of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Define a Deployment Step:\n",
    "ZenML makes it easy to define a deployment pipeline step that pushes your model into a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def deploy_model(model: ModelArtifact):\n",
    "    import pickle\n",
    "    # Save the model to a file (or deploy to a server)\n",
    "    with open('deployed_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Model deployed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run Deployment Step:\n",
    "You can chain the deployment step after model training and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model = deploy_model(model=tuned_model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits:\n",
    "- Automates the deployment process, ensuring consistent deployment configurations.\n",
    "- Ensures that the deployed model is the same version that was trained and tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing and Loading ZenML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Pipelines\n",
    "To store a ZenML pipeline, you need to use ZenML's built-in artifact stores and pipeline management features. This allows you to save the entire pipeline, including all the steps, inputs, and outputs, into an artifact store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Pipeline:\n",
    "Once you’ve defined all the steps in your pipeline (like loading data, processing data, training models, etc.), you need to compose them into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "\n",
    "# Define your pipeline\n",
    "@pipeline\n",
    "def churn_pipeline():\n",
    "    data = load_data()\n",
    "    processed_data = preprocess_data(data)\n",
    "    model = train_model(processed_data)\n",
    "    tuned_model = tune_model(processed_data)\n",
    "    deploy_model(tuned_model)\n",
    "\n",
    "# Run the pipeline\n",
    "churn_pipeline_instance = churn_pipeline()\n",
    "churn_pipeline_instance.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the Pipeline:\n",
    "You can store the pipeline by saving it to an artifact store. ZenML will automatically store the pipeline’s steps, configurations, and results in a persistent backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.core.repo import Repository\n",
    "\n",
    "# Store the pipeline\n",
    "repo = Repository()\n",
    "pipeline_id = repo.save_pipeline(churn_pipeline_instance)\n",
    "print(f\"Pipeline stored with ID: {pipeline_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step ensures that your pipeline, along with its steps and configurations, are saved in the ZenML repository and artifact store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pipelines\n",
    "Once you have stored your pipeline, you can load it from a different file, project, or environment. This is useful when you need to reuse the same pipeline configuration or when you are working across different environments (e.g., testing vs. production)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Pipeline:\n",
    "You can load an existing pipeline from the repository by using the load_pipeline function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.core.repo import Repository\n",
    "\n",
    "# Load the stored pipeline\n",
    "repo = Repository()\n",
    "pipeline_id = 'your-pipeline-id'  # Replace with the actual pipeline ID from the repository\n",
    "loaded_pipeline = repo.load_pipeline(pipeline_id)\n",
    "print(f\"Pipeline loaded with ID: {pipeline_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Loaded Pipeline:\n",
    "Once the pipeline is loaded, you can execute it just like you would with any other pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the loaded pipeline\n",
    "loaded_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will execute all the steps in the pipeline, using the exact configurations and data from the time it was originally saved."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
