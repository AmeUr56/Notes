{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Calculus for Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Calculus?\n",
    "Calculus is a branch of mathematics that studies how things change. It is divided into two main areas:\n",
    "\n",
    "- **Differential Calculus**: Deals with rates of change and slopes of curves.\n",
    "- **Integral Calculus**: Concerns accumulation of quantities and areas under curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Data Scientists Need Calculus\n",
    "In data science, calculus plays a key role in optimizing machine learning algorithms, such as finding the minimum of a cost function in regression, neural networks, and optimization problems. Calculus helps data scientists:\n",
    "\n",
    "- Understand how models change in response to input data.\n",
    "- Optimize algorithms (minimize loss functions using gradient descent).\n",
    "- Understand key concepts such as marginal changes, partial derivatives, and integrals in probabilistic models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts in Calculus for Data Science\n",
    "- **Limits**: The concept of approaching a value.\n",
    "- **Derivatives**: Measures of how a function changes as its input changes.\n",
    "- **Integrals**: The accumulation of quantities over an interval.\n",
    "- **Optimization**: Minimizing or maximizing a function.\n",
    "- **Gradient Descent**: A method for finding the minimum of a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Limits and Continuity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Limits\n",
    "The limit of a function describes the behavior of the function as x approaches a particular point. For example, the limit of \n",
    "**ùëì(ùë•)** as **ùë•** approaches 2 can be written as:<pre>\n",
    "lim ùëì(ùë•)\n",
    "ùë•‚Üí2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](./img/limit_at_a_point.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Limits through Examples\n",
    "If **ùëì(ùë•)=ùë•¬≤**, then the limit as **ùë•** approaches **2** is:<pre>\n",
    "lim ùë•¬≤ = 4\n",
    "ùë•‚Üí2</pre>\n",
    "This means that as **ùë•** gets closer to 2, **ùëì(ùë•)** approaches 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 4$"
      ],
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define the variable and function\n",
    "x = sp.Symbol('x')\n",
    "f = x**2\n",
    "\n",
    "# Calculate the limit as x approaches 2\n",
    "limit_f = sp.limit(f, x, 2)\n",
    "limit_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\infty$"
      ],
      "text/plain": [
       "oo"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.limit(f,x,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuity in Functions\n",
    "A function is continuous at a point if the limit exists at that point and equals the function‚Äôs value. In data science, continuity is important when working with optimization techniques and smooth loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function is not continuous at x = 2.\n",
      "Left limit: 4, Right limit: 5, Value at 2: nan\n",
      "The function is continuous at x = 5.\n"
     ]
    }
   ],
   "source": [
    "# Define the variable\n",
    "x = sp.symbols('x')\n",
    "\n",
    "# Define the function\n",
    "f = sp.Piecewise((x**2, x < 2), (2*x + 1, x > 2))\n",
    "\n",
    "# Function to check continuity at a given point\n",
    "def check_continuity(f, point):\n",
    "    # Evaluate the left-hand and right-hand limits of the function at the given point\n",
    "    left_limit = sp.limit(f, x, point, dir='-')\n",
    "    right_limit = sp.limit(f, x, point, dir='+')\n",
    "    \n",
    "    # Evaluate the value of the function at the point\n",
    "    value_at_point = f.subs(x, point)\n",
    "    \n",
    "    # Check if the left and right limits are equal and match the value at the point\n",
    "    if left_limit == right_limit == value_at_point:\n",
    "        print(f\"The function is continuous at x = {point}.\")\n",
    "    else:\n",
    "        print(f\"The function is not continuous at x = {point}.\")\n",
    "        print(f\"Left limit: {left_limit}, Right limit: {right_limit}, Value at {point}: {value_at_point}\")\n",
    "\n",
    "# Check continuity at x = 2\n",
    "check_continuity(f, 2)\n",
    "# Check continuity at x = 5\n",
    "check_continuity(f, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications in Data Science (Optimization)\n",
    "Limits are crucial in optimization. When tuning a machine learning model, you need to ensure that small changes in the model's parameters lead to small changes in the loss function, ensuring stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Concept and Definition\n",
    "The derivative of a function measures how it changes as its input changes. Mathematically, the derivative of a function **ùëì(ùë•)** at a point **ùë•** is:<pre>\n",
    "ùëì‚Ä≤(ùë•) = lim ùëì(ùë•+Œîùë•)‚àíùëì(ùë•) / Œîùë•\n",
    "_       Œîùë•‚Üí0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](./img/Slope_of_the_tangent_line.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Derivatives\n",
    "<pre>\n",
    "- ùëë/ùëëx ùë•**ùëõ = ùëõùë•**(ùëõ‚àí1)\n",
    "- ùëë/ùëëx sin(ùë•) = cos(ùë•)\n",
    "- ùëë/ùëëx ln‚Å°(ùë•) = 1/ùë•‚Äã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule, Product Rule, and Quotient Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule: \n",
    "<pre>\n",
    "ùëë/ùëëx ùëì(ùëî(ùë•)) = ùëì‚Ä≤(ùëî(ùë•))‚ãÖùëî‚Ä≤(ùë•)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Rule: \n",
    "<pre>\n",
    "ùëë/ùëëx [ùë¢(ùë•)ùë£(ùë•)] = ùë¢‚Ä≤(ùë•)ùë£(ùë•)+ ùë¢(ùë•)ùë£‚Ä≤(ùë•)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quotient Rule: \n",
    "<pre>\n",
    "ùëë/ùëëx [ùë¢(ùë•)/ùë£(ùë•)] = ùë¢‚Ä≤(ùë•)ùë£(ùë•)‚àíùë¢(ùë•)ùë£‚Ä≤(ùë•) / ùë£(ùë•)¬≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-Order Derivatives\n",
    "Higher-order derivatives, such as the second derivative \n",
    "**ùëì‚Ä≤‚Ä≤(ùë•)**, measure the rate of change of the rate of change and are essential in optimization for checking convexity and concavity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications in Data Science: Gradients, Cost Functions, and Optimization\n",
    "In machine learning, derivatives help minimize the cost function. The derivative, or gradient, provides the direction and rate of change of the function, which is used in gradient descent to optimize models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain Rule Example\n",
    "If **ùëì(ùë•) = sin(ùë•¬≤)**, the derivative using the chain rule is:<pre>\n",
    "ùëì‚Ä≤(ùë•)=cos(ùë•2)‚ãÖ2ùë•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x \\cos{\\left(x^{2} \\right)}$"
      ],
      "text/plain": [
       "2*x*cos(x**2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function\n",
    "g = x**2\n",
    "f = sp.sin(g)\n",
    "\n",
    "# Calculate the derivative using the chain rule\n",
    "f_prime = sp.diff(f, x)\n",
    "f_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code computes the derivative of **sin(ùë•¬≤)** using SymPy, applying the chain rule automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Partial Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept\n",
    "In multivariable calculus, a partial derivative measures how a function changes as one variable changes, keeping other variables constant. If **ùëì(ùë•,ùë¶)** is a function of two variables:<pre>\n",
    "‚àÇùëì / ‚àÇùë• , ‚àÇùëì / ‚àÇùë¶‚Äã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Vectors and Directional Derivatives\n",
    "The **gradient** of a function is a vector of partial derivatives, pointing in the direction of the steepest ascent. It is crucial in optimization for determining how to adjust parameters in machine learning models:<pre>\n",
    "‚àáùëì(ùë•,ùë¶) = (‚àÇùëì / ‚àÇùë• , ‚àÇùëì / ‚àÇùë¶)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](./img/partial_derivative_as_slope.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications in Machine Learning: Gradient Descent and Backpropagation\n",
    "Partial derivatives are fundamental in algorithms like gradient descent and backpropagation, where the goal is to adjust parameters in neural networks to minimize error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Gradient Descent\n",
    "Consider a function **ùëì(ùë•,ùë¶)=ùë•¬≤+ùë¶¬≤**. The partial derivatives are:<pre>\n",
    "‚àÇùëì / ‚àÇùë• = 2ùë•,‚àÇùëì / ‚àÇùë¶ = 2ùë¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 8.0, y: 5.92\n",
      "x: 6.4, y: 4.736\n",
      "x: 5.12, y: 3.7887999999999997\n",
      "x: 4.096, y: 3.03104\n",
      "x: 3.2768, y: 2.424832\n",
      "x: 2.62144, y: 1.9398655999999999\n",
      "x: 2.0971520000000003, y: 1.5518924799999998\n",
      "x: 1.6777216000000004, y: 1.2415139839999998\n",
      "x: 1.3421772800000003, y: 0.9932111871999998\n",
      "x: 1.0737418240000003, y: 0.7945689497599998\n",
      "x: 0.8589934592000003, y: 0.6356551598079998\n",
      "x: 0.6871947673600002, y: 0.5085241278463999\n",
      "x: 0.5497558138880001, y: 0.40681930227711993\n",
      "x: 0.43980465111040007, y: 0.32545544182169595\n",
      "x: 0.35184372088832006, y: 0.26036435345735676\n",
      "x: 0.281474976710656, y: 0.2082914827658854\n",
      "x: 0.22517998136852482, y: 0.16663318621270834\n",
      "x: 0.18014398509481985, y: 0.13330654897016667\n",
      "x: 0.14411518807585588, y: 0.10664523917613333\n",
      "x: 0.11529215046068471, y: 0.08531619134090666\n",
      "x: 0.09223372036854777, y: 0.06825295307272533\n",
      "x: 0.07378697629483821, y: 0.054602362458180266\n",
      "x: 0.05902958103587057, y: 0.043681889966544214\n",
      "x: 0.04722366482869646, y: 0.03494551197323537\n",
      "x: 0.037778931862957166, y: 0.027956409578588297\n",
      "x: 0.030223145490365734, y: 0.02236512766287064\n",
      "x: 0.024178516392292588, y: 0.01789210213029651\n",
      "x: 0.01934281311383407, y: 0.014313681704237208\n",
      "x: 0.015474250491067256, y: 0.011450945363389767\n",
      "x: 0.012379400392853806, y: 0.009160756290711813\n",
      "x: 0.009903520314283045, y: 0.007328605032569451\n",
      "x: 0.007922816251426436, y: 0.005862884026055561\n",
      "x: 0.006338253001141149, y: 0.004690307220844449\n",
      "x: 0.00507060240091292, y: 0.0037522457766755593\n",
      "x: 0.0040564819207303355, y: 0.0030017966213404476\n",
      "x: 0.0032451855365842686, y: 0.0024014372970723582\n",
      "x: 0.002596148429267415, y: 0.0019211498376578867\n",
      "x: 0.002076918743413932, y: 0.0015369198701263094\n",
      "x: 0.0016615349947311456, y: 0.0012295358961010474\n",
      "x: 0.0013292279957849164, y: 0.000983628716880838\n",
      "x: 0.001063382396627933, y: 0.0007869029735046703\n",
      "x: 0.0008507059173023465, y: 0.0006295223788037363\n",
      "x: 0.0006805647338418772, y: 0.0005036179030429891\n",
      "x: 0.0005444517870735017, y: 0.00040289432243439125\n",
      "x: 0.0004355614296588014, y: 0.000322315457947513\n",
      "x: 0.0003484491437270411, y: 0.0002578523663580104\n",
      "x: 0.00027875931498163285, y: 0.00020628189308640835\n",
      "x: 0.00022300745198530628, y: 0.00016502551446912668\n",
      "x: 0.00017840596158824503, y: 0.00013202041157530135\n",
      "x: 0.00014272476927059603, y: 0.00010561632926024107\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the gradient of the function\n",
    "def gradient(x, y):\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "# Implement gradient descent\n",
    "def gradient_descent(learning_rate=0.1, epochs=50):\n",
    "    x, y = 10.0, 7.4  # Starting point\n",
    "    for _ in range(epochs):\n",
    "        grad = gradient(x, y)\n",
    "        x, y = x - learning_rate * grad[0], y - learning_rate * grad[1]\n",
    "        print(f\"x: {x}, y: {y}\")\n",
    "\n",
    "gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code simulates gradient descent for the function \n",
    "**ùëì(ùë•,ùë¶) = ùë•2 + ùë¶2**, showing how the algorithm moves towards the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Differentiation for Vector-Valued Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian and Hessian Matrices\n",
    "- **Jacobian Matrix**: Represents first-order partial derivatives of vector-valued functions.\n",
    "- **Hessian Matrix**: Represents second-order partial derivatives. The Hessian is used to study the curvature of functions and determine if a critical point is a maximum, minimum, or saddle point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications in Machine Learning: Neural Networks and Multivariable Optimization\n",
    "Jacobian and Hessian matrices are used in neural networks for computing gradients in high-dimensional spaces, aiding in more accurate optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian Example\n",
    "Consider the function **ùëì(ùë•,ùë¶)=(ùë•¬≤+ùë¶,ùë•+ùë¶¬≤)**. The Jacobian matrix is:<pre>\n",
    "ùêΩ = \n",
    "_ ( 2ùë• 1\n",
    "_   1  2ùë¶)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2 x & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[2*x, 1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define variables and function\n",
    "x, y = sp.symbols('x y')\n",
    "f = x**2 + y\n",
    "\n",
    "# Compute the Jacobian matrix\n",
    "jacobian = sp.Matrix([f]).jacobian([x, y])\n",
    "jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Taylor Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taylor Expansions\n",
    "The Taylor series approximates functions around a specific point by using derivatives. For a function **ùëì(ùë•)** around **ùë• = ùëé**, the Taylor expansion is:<pre>\n",
    "ùëì(ùë•) ‚âà ùëì(ùëé) + ùëì‚Ä≤(ùëé)(ùë•‚àíùëé) + ùëì‚Ä≤‚Ä≤(ùëé)/2! * (ùë•‚àíùëé)¬≤ + ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](./img/taylor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Taylor Series for Function Approximation\n",
    "In machine learning, Taylor series can approximate non-linear functions, which is helpful in optimizations and model simplifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Taylor Expansion of sin(ùë•) around ùë•=0\n",
    "The Taylor series for **sin(ùë•)** is:<pre>\n",
    "sin(ùë•) ‚âà ùë• ‚àí ùë•¬≥/3! + ùë•‚Åµ/5! ‚àí..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x - \\frac{x^{3}}{6} + \\frac{x^{5}}{120} + O\\left(x^{6}\\right)$"
      ],
      "text/plain": [
       "x - x**3/6 + x**5/120 + O(x**6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taylor expansion of sin(x)\n",
    "taylor_exp = sp.series(sp.sin(x), x, 0, 6)\n",
    "taylor_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept\n",
    "Integration is the reverse process of differentiation. It calculates the accumulation of quantities, such as areas under curves.<pre>\n",
    "‚à´ ùëì(ùë•)‚Äâùëëùë•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definite vs Indefinite Integrals\n",
    "- **Indefinite Integral**: Represents the family of all antiderivatives.\n",
    "- **Definite Integral**: Computes the area under the curve from one point to another, with limits.<pre>\n",
    "‚à´a‚Üíb ùëì(ùë•)‚Äâùëëùë•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques of Integration\n",
    "- Substitution\n",
    "- Integration by parts\n",
    "- Partial fractions\n",
    "\n",
    "## Applications in Data Science: Probability, Area under Curves, and CDFs\n",
    "Integrals are used to compute areas under probability density functions, which is essential in statistics and machine learning for probabilistic models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Indefinite Integral\n",
    "Find the indefinite integral of **ùëì(ùë•)=ùë•¬≤**:<pre>\n",
    "‚à´ ùë•¬≤ ùëëùë• = ùë•¬≥ / 3 +ùê∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{x^{3}}{3}$"
      ],
      "text/plain": [
       "x**3/3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function\n",
    "f = x**2\n",
    "\n",
    "# Compute the indefinite integral\n",
    "integral_f = sp.integrate(f, x)\n",
    "integral_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Multivariable Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double and Triple Integrals\n",
    "In multivariable calculus, double and triple integrals extend the concept of integration to two and three dimensions, respectively.<pre>\n",
    "‚à´ùëé‚Üíùëè‚à´ùëê‚Üíùëë ùëì(ùë•,ùë¶)‚Äâùëëùë• dùë¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications in Data Science: Calculating Areas, Volumes, and Probabilities in Multiple Dimensions\n",
    "Multivariable integration is useful in complex probability distributions and calculating expected values in high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Double Integral\n",
    "Find the area under the surface **ùëì(ùë•,ùë¶)=ùë•+ùë¶** over the region **[0,1]√ó[0,1]**:<pre>\n",
    "‚à´0‚Üí1‚à´0‚Üí1 (ùë•+ùë¶) ùëëùë•‚Äâùëëùë¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1$"
      ],
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function\n",
    "f = x + y\n",
    "\n",
    "# Compute the double integral\n",
    "double_integral = sp.integrate(sp.integrate(f, (x, 0, 1)), (y, 0, 1))\n",
    "double_integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Optimization in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate and Multivariate Optimization\n",
    "In machine learning, optimization refers to the process of finding the minimum or maximum of a function, typically a cost function in models like linear regression or neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical Points and Saddle Points\n",
    "Critical points occur where the first derivative is zero. A critical point can be a local minimum, local maximum, or saddle point, determined by the second derivative (concavity or convexity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications in Data Science: Logistic Regression, SVMs, and Optimization Algorithms\n",
    "Optimization techniques like gradient descent are used to minimize loss functions in regression models, support vector machines (SVMs), and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Differential Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Definitions and Concepts\n",
    "Differential equations involve functions and their derivatives. They describe how a quantity changes over time or space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Differential Equations (ODEs) and Their Applications\n",
    "ODEs describe systems with one independent variable (e.g., time). They are used in time-series forecasting and dynamic models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications in Data Science: Time Series Forecasting, Dynamic Systems\n",
    "In data science, ODEs can model systems that evolve over time, such as predicting stock prices or modeling system dynamics in reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.integrate import odeint\n",
    "\n",
    "# Define a simple ODE dy/dt = -y\n",
    "def model(y, t):\n",
    "    dydt = -y\n",
    "    return dydt\n",
    "\n",
    "# Initial condition and time points\n",
    "y0 = 5\n",
    "t = np.linspace(0, 10, 100)\n",
    "\n",
    "# Solve ODE\n",
    "y = odeint(model, y0, t)\n",
    "\n",
    "# Plot the solution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(t, y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Calculus in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions and Gradient Descent\n",
    "The goal of machine learning algorithms is to minimize a loss function. Using derivatives (gradients), we iteratively update model parameters to minimize the error.<pre>\n",
    "ùúÉ = ùúÉ‚àíùõº‚àáùêΩ(ùúÉ)\n",
    "</pre>\n",
    "Where ùõº is the learning rate, ùúÉ are the model parameters, and ‚àáùêΩ(ùúÉ) is the gradient of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and the Role of Derivatives\n",
    "Regularization terms, such as **L1** and **L2**, are used to prevent overfitting. The derivatives of these terms are included in the gradient calculations.\n",
    "\n",
    "## Backpropagation in Neural Networks\n",
    "Backpropagation uses chain rules to propagate errors backward through the layers of a neural network, updating weights in the process.\n",
    "\n",
    "## Optimization Techniques and Convergence\n",
    "Beyond gradient descent, there are more advanced optimization techniques such as stochastic gradient descent (SGD), momentum, and Adam optimizer, which use derivatives to speed up convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Backprobagation:  [0.4 0.6]\n",
      "After Backprobagation:  [0.40743418 0.60371709]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulate a neural network backpropagation step\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Input data and weights\n",
    "inputs = np.array([1.0, 0.5])\n",
    "weights = np.array([0.4, 0.6])\n",
    "\n",
    "# Forward pass\n",
    "output = sigmoid(np.dot(inputs, weights))\n",
    "\n",
    "# Print weights before backpropagation\n",
    "print(\"Before Backprobagation: \",weights)\n",
    "\n",
    "# Backpropagation (simplified)\n",
    "error = output - 1  # Assume the true label is 1\n",
    "weights -= 0.1 * error * sigmoid_derivative(output) * inputs\n",
    "\n",
    "# Print weights after backpropagation\n",
    "print(\"After Backprobagation: \",weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
