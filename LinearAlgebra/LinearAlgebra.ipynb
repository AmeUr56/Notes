{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Definition of Linear Algebra**\n",
    "Linear algebra is a branch of mathematics that deals with vectors, matrices, and linear transformations. It focuses on the study of vector spaces (also called linear spaces) and the linear equations that define relationships between elements in these spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Purpose of Linear Algebra**:\n",
    " Linear algebra forms the mathematical backbone of many machine learning algorithms, from basic linear regression to complex neural networks and dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Linear Algebra?**:<br>\n",
    "\n",
    "- **Data Representation**: Vectors and matrices are fundamental structures for representing datasets, with rows as samples and columns as features.\n",
    "\n",
    "- **Transformations**: Linear algebra helps in applying geometric transformations (e.g., rotations, scaling), crucial for feature engineering and understanding data relationships.\n",
    "\n",
    "- **Dimensionality Reduction**: Techniques like Principal Component Analysis (PCA) rely on linear algebra (eigenvectors/eigenvalues) to reduce the complexity of data while retaining its essence.\n",
    "\n",
    "- **Optimization**: Many machine learning models use optimization algorithms (e.g., gradient descent) which are based on solving systems of linear equations.\n",
    "\n",
    "- **Neural Networks and Deep Learning**: Operations in deep learning, such as computing weights and activations, are expressed as matrix multiplications and vector operations.\n",
    "\n",
    "- **Efficiency**: Linear algebra allows for efficient computation of large datasets, enabling faster model training and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of linear algebra and machine learning, data types represent different mathematical structures used to organize and manipulate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Scalars**:\n",
    "- **Definition**: A single number, often representing a simple value.\n",
    "- **Example**: 5, 3.14, -2.\n",
    "- **Use in ML**: Scalars can represent individual coefficients, weights, or biases in models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- **Constant Symbols**: Œ±, Œ≤, Œª, œÉ\n",
    "- **Variable Symbols**: a, b, c, ..., y, z\n",
    "- Can be Real or Complex, but in ML only Real : a ‚àà ‚Ñù\n",
    "- Models's Hyperparameter are usually scalars. \n",
    "- Scalars are simple but important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](img/scalar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Vectors**:\n",
    "- **Definition**: An ordered list of numbers(scalars) (1D array) that represent a point in space or a set of features.\n",
    "- **Example**: \n",
    "ùë£\n",
    "=\n",
    "[\n",
    "ùë£1\n",
    ",\n",
    "ùë£2\n",
    ",\n",
    "...\n",
    ",\n",
    "ùë£n\n",
    "].<br>\n",
    "- **Use in ML**: Features of a dataset or weights of a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Notes**:\n",
    "- Horizontal vector is called Row Vector.\n",
    "- Vertical vector is called Column Vector.\n",
    "- Transpose operation converts row vector to column and viceversa.\n",
    "- Vector Symbols are the bold copy of Scalar Symbols\n",
    "- Models Features(inputs) are usually vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](img/vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Matrices**:\n",
    "- **Definition**: \n",
    "    - A rectangular array of numbers arranged in rows and columns (2D array).\n",
    "    - Collection of Vectors.\n",
    "- **Notation**:\n",
    "A = [aij]m*n\n",
    "    - **A** is the matrix.\n",
    "    - **a** is element of the matrix.\n",
    "    - **i** is the index to the matrix's row.\n",
    "    - **j** is the index to the matrix's column.\n",
    "    - **m** is number of rows.\n",
    "    - **n** is number of cols.\n",
    "- **Example**:<br>\n",
    "ùê¥ =<br>\n",
    "[\n",
    "**a**11\n",
    "**a**12\n",
    "**a**13\n",
    "...\n",
    "**a**1n<br>\n",
    "**a**21\n",
    "**a**22\n",
    "**a**23\n",
    "...\n",
    "**a**2n<br>\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "...<br>\n",
    "**a**m1\n",
    "**a**m2\n",
    "**a**m3\n",
    "...\n",
    "**a**mn\n",
    "]\n",
    "\n",
    "- **Use in ML**: Represent datasets (rows as samples, columns as features) or transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Notes**:\n",
    "- row of matrix is a vector of size m.\n",
    "- column of matrix is vector of size n.\n",
    "- list of elements from top left to bottom right(**a**11 to **a**mn) are known as **diagonal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](img/matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Tensors**:\n",
    "- **Definition**: A generalization of vectors and matrices to higher dimensions (n-dimensional arrays).\n",
    "- **Example**: A 3D tensor could represent a color image where each pixel has RGB values.\n",
    "- **Use in ML**: Tensors are widely used in deep learning frameworks like TensorFlow to store multidimensional data such as images, videos, or batches of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Notes**:\n",
    "- Tensors are used too much in DL like image representation RGB.\n",
    "- Tensors are Generalized representation of Scalars, Vectors, Matrices:\n",
    "    - Scalar is 0-dim tensor.\n",
    "    - Vector is 1-dim tensor.\n",
    "    - Matrix is 2-dim tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](img/tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Vector Geometric Prespective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Representation of a Vector**\n",
    "A vector is usually represented as an arrow in a space:\n",
    "- **Starting point**: Called the tail (often the origin).\n",
    "- **Ending point**: Called the head.\n",
    "<br>\n",
    "\n",
    "**Example** in 2D: A vector ùë£=[3,2]\n",
    "can be visualized as an arrow from the origin (0,0) to the point (3,2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](img/vector_repr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Magnitude (Length) & Direction of a Vector**\n",
    "The magnitude (or norm) of a vector is its length, calculated using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Norm:\n",
    "The L1 norm is the sum of the absolute values of the vector components.<br>\n",
    "- **Geometric Interpretation**: It represents the distance between points if you can only move along the grid (like in a city grid system).\n",
    "- **Use in ML**: L1 regularization (Lasso) encourages sparsity in machine learning models by driving some coefficients to zero.\n",
    "\n",
    "![Alt Text](img/l1norm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Norm:\n",
    "The L2 norm is the square root of the sum of the squared components of the vector.\n",
    "- **Geometric Interpretation**: It represents the straight-line (Euclidean) distance from the origin to the point in space.\n",
    "- **Use in ML**: L2 regularization (Ridge) penalizes large coefficients and helps prevent overfitting by making the solution smoother.\n",
    "\n",
    "![Alt Text](img/l2norm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Norm:\n",
    "The L‚àû norm is the maximum absolute value of the vector components.\n",
    "- **Geometric Interpretation**: It gives the largest distance along any single axis.\n",
    "- **Use in ML**: It is used in optimization problems where you want to limit the largest deviation or outliers.\n",
    "\n",
    "![ALT TEXT](img/maxnorm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **p-norm** (also called the Lp-norm) is a generalized way to define the length or size of a vector for any positive integer \n",
    "ùëù.<br><br>\n",
    "![Alt Text](img/pnorm_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Unit Vector**\n",
    "A unit vector is a vector that has a magnitude (length) of 1. It is typically used to indicate direction without scaling the vector's length. A unit vector in the direction of any vector can be obtained by normalizing that vector (i.e., dividing the vector by its magnitude)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula:\n",
    "If ùë£ is a vector, the corresponding **unit vector ùë£^** is given by:<br><pre>\n",
    "ùë£^ = ùë£ / ‚à£‚à£ùë£‚à£‚à£</pre>\n",
    "Where:\n",
    "- **ùë£** is the original vector.\n",
    "- **‚à£‚à£ùë£‚à£‚à£** is the magnitude (or norm) of the vector **V**.\n",
    "- **ùë£^** is the resulting unit vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "- For a vector ùë£=[3,4] the magnitude is:<pre>\n",
    "‚à£‚à£ùë£‚à£‚à£=sqrt(3¬≤+4¬≤)=sqrt(9+16)=5\n",
    "- The unit vector ùë£^ is:<pre>\n",
    "ùë£^ =[3,4]/5=[3/5,4/5]=[0.6,0.8]</pre>\n",
    "\n",
    "This new vector [0.6,0.8] has a magnitude of 1, pointing in the same direction as ùë£."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape created by different norms:\n",
    "is important because it reflects how distances and magnitudes are measured in various contexts, particularly in machine learning, optimization, and data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1 Norm\n",
    "![ALT TEXT](img/l1_unit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L2 Norm\n",
    "![ALT TEXT](img/l2_unit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Stretching & Shrinking Vectors**\n",
    "In linear algebra, stretching and shrinking refer to changing the magnitude of a vector without altering its direction.\n",
    "\n",
    "- **Stretching**: A vector is stretched when it is **multiplied by a scalar** greater than 1. This **increases its magnitude** (length).<pre>\n",
    "ùë£‚Ä≤= ùëê‚ãÖùë£ where ùëê >1 \n",
    "\n",
    "- **Shrinking**: A vector is shrunk when **multiplied by a scalar** between 0 and 1, **reducing its magnitude**.<pre>\n",
    "ùë£‚Ä≤= ùëê‚ãÖùë£ where 0 < ùëê < 1 \n",
    "\n",
    "#### Key Point:\n",
    "- The direction can be changed by multiplication by a **negative scalar**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Rotating a Vector (Linear Combination of Vectors)**\n",
    "In linear algebra, a vector can be expressed as a linear combination of other vectors. When we talk about rotating a vector, we often think of the operation in terms of transforming it into a new vector using other vectors.\n",
    "\n",
    "\n",
    "- A vector **ùë£** can be expressed as a linear combination of two basis vectors **ùëé** and **ùëè**:<pre>\n",
    "ùë£ = ùëê1\\*ùëé + ùëê2\\*ùëè\n",
    "\n",
    "where ùëê1‚Äã and ùëê2 are scalar coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Span of a Set of Vectors**\n",
    "The span of a set of vectors is a fundamental concept in linear algebra that describes all possible linear combinations of those vectors. It represents a subspace formed by combining the vectors in various ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](img/span.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Definition\n",
    "The span of a set of vectors \n",
    "{ùë£1,ùë£2,...,ùë£ùëõ} is defined as:<pre>\n",
    "Span({ùë£1,ùë£2,...,ùë£ùëõ})={ùëê1ùë£1+ùëê2ùë£2+...+ùëêùëõùë£ùëõ ‚à£ ùëê1,ùëê2,...,ùëêùëõ ‚àà ùëÖ}</pre>\n",
    "\n",
    "#### 2. Geometric Interpretation\n",
    "  - **In 2D**: The span of two non-collinear vectors will form the entire 2D plane.\n",
    "  - **In 3D**: The span of three non-coplanar vectors will fill the entire 3D space.\n",
    "  - If vectors are linearly dependent, the span will be a lower-dimensional subspace (e.g., a line or plane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Basis Vectors**\n",
    "Basis vectors are a set of vectors in a vector space that are used to describe all other vectors in that space through linear combinations. They play a crucial role in linear algebra, enabling the representation of vectors in a systematic and efficient manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](img/2d_basis_vecs.png) ![ALT TEXT](img/3d_basis_vecs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Definition\n",
    "A set of vectors \n",
    "{ùëè1,ùëè2,...,ùëèùëõ} is called a basis for a vector space ùëâ if:\n",
    "- **Spanning**: The span of the basis vectors equals the vector space ùëâ:<pre>\n",
    "Span({ùëè1,ùëè2,‚Ä¶,ùëèùëõ})=ùëâ</pre>\n",
    "\n",
    "- **Linear Independence**: The vectors are linearly independent, meaning no vector in the set can be expressed as a linear combination of the others.\n",
    "\n",
    "#### 2. Properties\n",
    "- The number of vectors in a basis corresponds to the **dimension** of the vector space. For example:\n",
    "    - A basis for ùëÖ¬≤ consists of 2 vectors.\n",
    "    - A basis for ùëÖ¬≥ consists of 3 vectors.\n",
    "- Any vector in the space can be expressed as a unique linear combination of the basis vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Dot Product**\n",
    "The dot product (also known as the scalar product or inner product) is a fundamental operation in linear algebra that takes two vectors and produces a scalar (a single number). It is widely used in various fields, including physics, engineering, and machine learning, to measure the angle between vectors, projection, and similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](img/dot_product.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Definition\n",
    "For two vectors  ùëé and b in ùëÖ‚Åø, the dot product is defined as:<pre>\n",
    "ùëé‚ãÖùëè=ùëé1ùëè1+ùëé2ùëè2+...+ùëéùëõùëèùëõ</pre>\n",
    "where **ùëéùëñ** and **ùëèùëñ** are the components of vectors **ùëé** and **ùëè**,respectively.\n",
    "\n",
    "#### 2. Geometric Interpretation\n",
    "The dot product can also be expressed in terms of the magnitudes of the vectors and the cosine of the angle Œ∏ between them:<pre>\n",
    "ùëé‚ãÖùëè=‚à•ùëé‚à•‚à•ùëè‚à•cos(ùúÉ)</pre>\n",
    "where:\n",
    "- **‚à•ùëé‚à•** and **‚à•ùëè‚à•** are the magnitudes (lengths) of the vectors.\n",
    "- **Œ∏** is the angle between the vectors.\n",
    "\n",
    "#### 3. Properties\n",
    "- **Commutative** <pre>:ùëé‚ãÖùëè = ùëè‚ãÖùëé</pre>\n",
    "- **Distributive**: <pre>ùëé‚ãÖ(ùëè+ùëê) = ùëé‚ãÖùëè+ùëé‚ãÖùëê</pre>\n",
    "- **Associative with Scalars**:<pre>(ùëêùëé)‚ãÖùëè=ùëê(ùëé‚ãÖùëè)</pre> for¬†any¬†scalar¬†ùëê\n",
    "- **Dot Product Between Basis Vectors**:The dot product of orthonormal basis vectors is 0 if the vectors are different, and 1 if they are the same:<pre>\n",
    "e_i ¬∑ e_j = \n",
    "{\n",
    "    1 if i = j\n",
    "    0 if i ‚â† j\n",
    "}\n",
    "![ALT TEXT](img/dot_product_basis.png)\n",
    "- **Dot Product Between Vector & Itself**:The dot product of a vector with itself gives the square of its magnitude:<pre>\n",
    "v ¬∑ v = ||v||^2\n",
    "![ALT TEXT](img/dot_product_itself.png)\n",
    "- **How the dot product affects the angle**:\n",
    "    - If ùëé‚ãÖùëè>0(positive), the angle between ùëé and ùëè is acute (less than 90¬∞).\n",
    "    - If ùëé‚ãÖùëè=0, the angle between ùëé and ùëè is 90¬∞ (the vectors are orthogonal).\n",
    "    - If ùëé‚ãÖùëè<0 (negative), the angle between ùëé and ùëè is obtuse (greater than 90¬∞).\n",
    "\n",
    "#### 4. Applications\n",
    "- **Angle Between Vectors**: The dot product can be used to find the cosine of the angle between two vectors. If ùëé‚ãÖùëè=0, then the vectors are orthogonal (perpendicular).\n",
    "- **Projection**: The dot product is used to project one vector onto another:<pre>\n",
    "Proj ùëè(ùëé)= (ùëé‚ãÖùëè / ‚à•ùëè‚à•) *ùëè\n",
    "- **Similarity Measurement**: In machine learning, the dot product is often used in algorithms like Support Vector Machines (SVMs) and in calculating cosine similarity for text and document comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Orthogonalization & Gram-Schmidt Algorithm**\n",
    "### 1. Orthogonalization\n",
    "Orthogonalization is the process of converting a set of vectors into an orthogonal set. This means transforming a collection of non-orthogonal vectors into vectors that are perpendicular (orthogonal) to each other. In machine learning and data science, orthogonalization can improve the numerical stability of algorithms by eliminating redundancy and dependencies among vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Gram-Schmidt Algorithm\n",
    "The **Gram-Schmidt process** is a widely used method for **orthogonalizing a set of vectors**. It takes a linearly independent set of vectors and transforms them into an orthogonal (or orthonormal) set while preserving the span of the original vectors.\n",
    "\n",
    "#### Steps of the Gram-Schmidt Algorithm\n",
    "Let‚Äôs assume we have a set of vectors {ùë£1,ùë£2,...,ùë£ùëõ}.\n",
    "\n",
    "- **Start with the first vector**:\n",
    "Set the first orthogonal vector **ùë¢1=ùë£1**.\n",
    "\n",
    "- **Orthogonalize each subsequent vector**: For each vector **ùë£ùëò**, subtract the projections onto the previous orthogonal vectors to ensure orthogonality:<pre>\n",
    "ùë¢ùëò=ùë£ùëò‚àí‚àëùëñ=1ùëò‚àí1Proj ùë¢ùëñ ùë£ùëò </pre>\n",
    "\n",
    "- **Normalize (optional)**:\n",
    "If an orthonormal set is desired, normalize each ùë¢ùëò to get unit vectors:\n",
    "ùëíùëò = ùë¢ùëò / ‚à£‚à£ùë¢ùëò‚à£‚à£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](img/schmidt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. System of Linear Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Equation of Line**\n",
    "The equation of a line in a 2D plane can be written in several forms. The general one is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Form:\n",
    "The general form of a line is:<pre>\n",
    "Ax+By+C=0</pre>\n",
    "Where ùê¥, ùêµ, and ùê∂ are constants.\n",
    "![ALT TEXT](img/line.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of Linear Equations\n",
    "- Natural phenomenances can be modeled using linear equation.\n",
    "- Complex functions apear linear when viewed in small enough scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](img/import_line.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. System of Linear Equations**\n",
    "A **system of linear equations** consists of multiple linear equations that share common variables. The objective is to find values for these variables that satisfy all equations in the system simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Representation\n",
    "A system of **ùëö** linear equations in **ùëõ** variables can be expressed as:<pre>\n",
    "ùëé11ùë•1+ùëé12ùë•2+ ... +ùëé1ùëõùë•ùëõ=ùëè1\n",
    "ùëé21ùë•1+ùëé22ùë•2+ ... +ùëé2ùëõùë•ùëõ=ùëè2\n",
    "‚ãÆ\n",
    "ùëéùëö1ùë•1+ùëéùëö2ùë•2+ ... +ùëéùëöùëõùë•ùëõ=ùëèùëö</pre> \n",
    "Where:\n",
    "- **ùëéùëñùëó**‚Äã are coefficients,\n",
    "- **ùë•ùëó** are the variables, and\n",
    "- **ùëèùëñ** are constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Solutions\n",
    "- **Unique Solution**: The system has exactly one solution. This occurs when the equations are independent, and the corresponding matrix ùê¥ is invertible.\n",
    "\n",
    "- **Infinite Solutions**: The system has infinitely many solutions when at least one equation can be derived from others (dependent equations).\n",
    "\n",
    "- **No Solution**: The system has no solution when the equations are contradictory (inconsistent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](img/sys_lines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution of System of Linear Equations\n",
    "There are several methods for solving a system of linear equations, each suited for different types of systems and contexts. Here are some common methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Substitution Method\n",
    "- Solve one equation for one variable, and then substitute that expression into the other equations. This method works well for systems with a small number of equations and variables.\n",
    "![ALT TEXT](img/substitution_method.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Elimination Method\n",
    "Also known as the Gaussian elimination method, this approach involves adding or subtracting equations to eliminate variables systematically. The goal is to reduce the system to an upper triangular form and then solve using back substitution.<br>\n",
    "![ALT TEXT](img/elimination_method.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can every set of equations be solved?\n",
    "- If the nubmer of unknows is more than the number of equations.\n",
    "- If the given equations are not independent pieces of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Linear Equation as Vector Dot Multiplication**\n",
    "A **linear equation** in ùëõ variables can be expressed as a dot product of two vectors:<pre>\n",
    "ùëé1ùë•1 + ùëé2ùë•2 + ‚ãØ + ùëéùëõùë•ùëõ = ùëè</pre>\n",
    "\n",
    "This is equivalent to the dot product of the coefficient vector \n",
    "ùëé=[ùëé1,ùëé2,‚Ä¶,ùëéùëõ] and the variable vector \n",
    "ùë•=[ùë•1,ùë•2,‚Ä¶,ùë•ùëõ], resulting in:<pre>\n",
    "ùëé‚ãÖùë•=ùëè</pre>\n",
    "\n",
    "Where **ùëè** is the constant on the right-hand side. The dot product form simplifies understanding of the geometric interpretation of linear equations, making it easier to visualize in higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](img/matrix_dot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **4. System of Linear Equations as Matrix-Vector Multiplication**\n",
    "A system of linear equations can be expressed as matrix-vector multiplication. Consider the system:<pre>\n",
    "ùëé11ùë•1+ùëé12ùë•2+ ... +ùëé1ùëõùë•ùëõ=ùëè1\n",
    "ùëé21ùë•1+ùëé22ùë•2+ ... +ùëé2ùëõùë•ùëõ=ùëè2\n",
    "‚ãÆ\n",
    "ùëéùëö1ùë•1+ùëéùëö2ùë•2+ ... +ùëéùëöùëõùë•ùëõ=ùëèùëö</pre> \n",
    "This system can be represented as:<pre>\n",
    "ùê¥ùë•=ùëè</pre>\n",
    "Where:\n",
    "- **ùê¥** is the coefficient matrix of size **ùëö√óùëõ**.\n",
    "- **ùë•** is the variable vector of size **ùëõ√ó1**.\n",
    "- **ùëè** is the constant vector of size **ùëö√ó1**.\n",
    " \n",
    "This compact representation is useful for solving and analyzing systems of linear equations using methods like matrix inversion, Gaussian elimination, or numerical techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](img/matrix_dot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Gauss-Jordan Elimination**\n",
    "Gauss-Jordan Elimination is a method used to solve systems of linear equations by transforming the augmented matrix into **reduced row echelon form (RREF)**. This process eliminates the need for back substitution and provides a straightforward path to find the solution for the variables.\n",
    "\n",
    "### Steps of Gauss-Jordan Elimination:\n",
    "1. **Form the Augmented Matrix**: Write the system of linear equations as an augmented matrix \\( [A|b] \\), where \\( A \\) is the coefficient matrix and \\( b \\) is the constants vector.\n",
    "   \n",
    "2. **Row Operations**: Perform the following elementary row operations:\n",
    "   - Swap two rows.\n",
    "   - Multiply a row by a nonzero scalar.\n",
    "   - Add or subtract a multiple of one row from another row.\n",
    "   \n",
    "3. **Transform to Row Echelon Form (REF)**:\n",
    "   - Start by making the leading coefficient (pivot) in the first row, first column equal to 1 (if not already), by scaling the row.\n",
    "   - Use the pivot to eliminate all other entries in the column below it by adding/subtracting suitable multiples of the pivot row from other rows.\n",
    "   \n",
    "4. **Transform to Reduced Row Echelon Form (RREF)**:\n",
    "   - Make the pivot entry in each row equal to 1.\n",
    "   - Use the pivot to eliminate all other entries in its column, both above and below.\n",
    "   \n",
    "5. **Extract the Solution**:\n",
    "   - Once the matrix is in RREF, the solution to the system can be easily extracted from the matrix.\n",
    "\n",
    "#### Applications:\n",
    "- Gauss-Jordan Elimination is widely used for solving linear systems in fields such as machine learning, optimization problems, and computational mathematics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Rank of a Matrix**\n",
    "The **rank** of a matrix is defined as the maximum number of linearly independent rows or columns in the matrix. It gives an indication of how much information the matrix holds and is closely related to the concept of linear independence in vector spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points:\n",
    "- **Row Rank**: The maximum number of linearly independent rows in the matrix.\n",
    "- **Column Rank**: The maximum number of linearly independent columns in the matrix.\n",
    "- **Fundamental Theorem of Ranks**: In any matrix, the row rank and column rank are always equal, so they both are referred to simply as the **rank of the matrix**.\n",
    "- The rank of a matrix provides insight into the **dimensionality** of the vector space spanned by its rows or columns.\n",
    "- A matrix with full rank has all its rows or columns linearly independent.\n",
    "\n",
    "#### How to Find the Rank:\n",
    "1. **Gaussian Elimination**: Use Gaussian or Gauss-Jordan elimination to reduce the matrix to **row echelon form** or **reduced row echelon form (RREF)**. The number of non-zero rows in this form is the rank of the matrix.\n",
    "   \n",
    "2. **Determinant Method** (for square matrices): The rank of a matrix is the largest order of any non-zero **determinant** of its square submatrices.\n",
    "### Applications of Matrix Rank:\n",
    "- **Solving Linear Systems**: The rank helps determine if a system of linear equations has a unique solution, infinitely many solutions, or no solution at all.\n",
    "- **Dimensionality Reduction**: Rank is used in techniques like Principal Component Analysis (PCA) to reduce the dimensionality of data while preserving as much information as possible.\n",
    "- **Linear Transformations**: The rank tells us about the mapping power of a linear transformation, indicating how many dimensions of the output space are covered by the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Types of Matrices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Square Matrix**: A matrix with the same number of rows and columns nxn.<br>\n",
    "![ALT TEXT](img/square.png)\n",
    "\n",
    "- **Rectangular Matrix**: A matrix where the number of rows is different from the number of columns mxn.<br>\n",
    "![ALT TEXT](img/rectangle.png)\n",
    "\n",
    "- **Symmetric Matrix**: A square matrix that is equal to its transpose, i.e.,A = A^T.<br>\n",
    "![ALT TEXT](img/symmetric.png)\n",
    "\n",
    "- **Diagonal Matrix**: A square matrix where all non-diagonal elements are zero. The diagonal elements can be non-zero.<br>\n",
    "![ALT TEXT](img/diagonal.png)\n",
    "\n",
    "- **Identity Matrix**: A diagonal matrix where all diagonal elements are 1. It acts as the multiplicative identity in matrix multiplication.<br>\n",
    "![ALT TEXT](img/identity.png)\n",
    "\n",
    "- **Upper Triangular Matrix**: A square matrix where all elements below the main diagonal are zero.<br>\n",
    "![ALT TEXT](img/upper_tr.png)\n",
    "\n",
    "- **Lower Triangular Matrix**: A square matrix where all elements above the main diagonal are zero.<br>\n",
    "![ALT TEXT](img/lower_tr.png)\n",
    "\n",
    "- **Orthogonal Matrix**: A square matrix where the rows and columns are orthonormal vectors, i.e., \\( A \\cdot A^T = I \\), where \\( I \\) is the identity matrix.<br>\n",
    "![ALT TEXT](img/orthogonal.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Transpose of a Matrix**\n",
    "The **transpose** of a matrix is obtained by swapping its rows and columns. If **A** is an **mxn** matrix, its transpose, denoted by **A^T**, is an **nxm** matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](img/transpose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Types of Matrices by Definiteness**\n",
    "\n",
    "- **Positive Definite Matrix**: A symmetric matrix \\( A \\) is positive definite if for all non-zero vectors \\( x \\), the following condition holds:\n",
    "  \\[\n",
    "  x^T A x > 0\n",
    "  \\]\n",
    "\n",
    "- **Positive Semidefinite Matrix**: A symmetric matrix \\( A \\) is positive semidefinite if for all vectors \\( x \\):\n",
    "  \\[\n",
    "  x^T A x \\geq 0\n",
    "  \\]\n",
    "\n",
    "- **Negative Definite Matrix**: A symmetric matrix \\( A \\) is negative definite if for all non-zero vectors \\( x \\):\n",
    "  \\[\n",
    "  x^T A x < 0\n",
    "  \\]\n",
    "\n",
    "- **Negative Semidefinite Matrix**: A symmetric matrix \\( A \\) is negative semidefinite if for all vectors \\( x \\):\n",
    "  \\[\n",
    "  x^T A x \\leq 0\n",
    "  \\]\n",
    "\n",
    "- **Indefinite Matrix**: A symmetric matrix \\( A \\) is indefinite if there exists at least one vector \\( x \\) such that:\n",
    "  \\[\n",
    "  x^T A x > 0\n",
    "  \\]\n",
    "  and at least one vector \\( y \\) such that:\n",
    "  \\[\n",
    "  y^T A y < 0\n",
    "  \\]\n",
    "\n",
    "![ALT TEXT](img/def.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Matrix Transformation on Vectors**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Identity Matrix on a Vector**:\n",
    "    - The identity matrix **ùêºùëõ** is a square matrix with ones on the diagonal and zeros elsewhere.\n",
    "    - When the identity matrix multiplies a vector **ùë•**, it leaves the vector unchanged:<pre> ùêºùëõùë• = ùë•\n",
    "\n",
    "![ALT TEXT](img/identity_vector.png)\n",
    "\n",
    "- **Scaled Identity Matrix on a Vector**:\n",
    "    - A scaled identity matrix is the identity matrix multiplied by a scalar **ùúÜ**, denoted as **ùúÜùêºùëõ**.\n",
    "    - When a scaled identity matrix multiplies a vector ùë•, it scales the vector by ùúÜ:<pre> ùúÜùêºùëõùë• = ùúÜùë•\n",
    "\n",
    "    - Every component of the vector is multiplied by the scalar ùúÜ, effectively scaling the entire vector by that factor.\n",
    "    \n",
    "![ALT TEXT](img/scaled_identity_vector.png)\n",
    "\n",
    "- **Diagonal Matrix on a Vector**:\n",
    "    - A diagonal matrix ùê∑ has arbitrary values on the diagonal and zeros elsewhere.\n",
    "    - When a diagonal matrix multiplies a vector ùë•, each component of the vector is scaled by the corresponding diagonal element.\n",
    "    - Each element of the vector is multiplied by the corresponding diagonal value of the matrix.\n",
    "\n",
    "![ALT TEXT](img/diagonal_vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Matrix Transformation on Space**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Identity matrix**\n",
    "\n",
    "![ALT TEXT](img/identity_space.png)\n",
    "\n",
    "- **Diagonal matrix**\n",
    "\n",
    "![ALT TEXT](img/diagonal_space.png)\n",
    "\n",
    "- **Orthogonal matrix**\n",
    "\n",
    "![ALT TEXT](img/orthogonal_space.png)\n",
    "\n",
    "- **Symmetric matrix**\n",
    "\n",
    "![ALT TEXT](img/symmetric_space.png)\n",
    "\n",
    "    - When two rows are in same direction.\n",
    "\n",
    "![ALT TEXT](img/symmetric_space1.png)\n",
    "\n",
    "- **Upper Triangular matrix**\n",
    "\n",
    "![ALT TEXT](img/up_tr_space.png)\n",
    "\n",
    "- **Lower Triangular matrix**\n",
    "\n",
    "![ALT TEXT](img/lw_tr_space.png)\n",
    "\n",
    "- **Positive Definite matrix**\n",
    "\n",
    "![ALT TEXT](img/pos_def_space.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Matrix Inverse**\n",
    "The inverse of a matrix is a matrix that, when multiplied by the original matrix, results in the identity matrix. Not all matrices have inverses‚Äîonly square matrices that are non-singular (i.e., their determinant is non-zero) have inverses.<br>\n",
    "\n",
    "If ùê¥ is a square matrix, its inverse is denoted as ùê¥‚àí1, and the following holds:<pre>\n",
    "\n",
    "ùê¥ ùê¥‚àí1=ùê¥‚àí1 ùê¥=ùêºùëõ</pre>\n",
    "\n",
    "Where:\n",
    "- **ùê¥** is an ùëõ√óùëõ matrix.\n",
    "- **ùê¥‚àí1**  is the inverse of ùê¥.\n",
    "- **ùêºùëõ** is the ùëõ√óùëõ identity matrix.\n",
    "\n",
    "### Key Points:\n",
    "- **Existence**: Not all matrices have an inverse. A matrix must be square and have a non-zero determinant to have an inverse.\n",
    "- **Uniqueness**: If the inverse exists, it is unique.\n",
    "Multiplicative Identity: The product of a matrix and its inverse is the identity matrix.\n",
    "- **Applications**: Matrix inverses are used in solving systems of linear equations, particularly in the equation **ùê¥ùë•=ùëè**, where **ùë•=ùê¥‚àí1 ùëè**.\n",
    "\n",
    "![ALT TEXT](img/inv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orthogonal Matrix Inverse:\n",
    "![ALT TEXT](img/orthog_inv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Determinants "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Geometric Areas and Volumes with Vectors in 2D and 3D Space**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Area of Parallelogram Represented by Basic Vectors of 2D Space**:\n",
    "In 2D space, the area of a parallelogram formed by two basic vectors ùëé and ùëè is given by the absolute value of their determinant:<pre>\n",
    "Area=|ùëé1ùëè2‚àíùëé2ùëè1‚à£\n",
    "\n",
    "![ALT TEXT](img/2d_basis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Area of Parallelogram Represented by Linearly Dependent Vectors**:\n",
    "- If the vectors are linearly dependent, the area of the parallelogram collapses to zero because the vectors are **collinear**, representing no \"spread\" in space.\n",
    "\n",
    "![ALT TEXT](img/2d_dep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Volume of Parallelogram Represented by Basic Vectors of 3D Space:**\n",
    "In 3D, the area of the parallelogram formed by two vectors ùëé and ùëè is given by the magnitude of their cross product:<pre>\n",
    "Area=‚à£ùëé√óùëè‚à£\n",
    "\n",
    "![ALT TEXT](img/3d_basis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.Volume of Parallelepiped Represented by Linearly Dependent Vectors:**\n",
    "In 3D, the volume of a parallelepiped formed by three vectors ùëé,ùëè,ùëê is zero if the vectors are linearly dependent, as they lie in the same plane:<pre>\n",
    "Volume=‚à£ùëé‚ãÖ(ùëè√óùëê)‚à£=0\n",
    "\n",
    "![ALT TEXT](img/3d_dep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Scaling of Geometric Shapes**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. How a Matrix Scales the Area of the Parallelogram:**\n",
    "\n",
    "![ALT TEXT](img/2d_scale.png)\n",
    "\n",
    "### **2. How a Matrix Scales the Volume of the Parallelepiped:**\n",
    "\n",
    "![ALT TEXT](img/3d_scale.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Calculate the Determinant of a Matrix:**\n",
    "### For a 2x2 matrix:<pre>\n",
    "ùê¥=( ùëé  ùëè \n",
    "    ùëê  ùëë)</pre>\n",
    "- The determinant is calculated as:<pre>\n",
    "det(ùê¥) = ùëéùëë‚àíùëèùëê</pre>\n",
    "\n",
    "### For a 3x3 matrix:<pre>\n",
    "ùê¥=(ùëé ùëè ùëê \n",
    "ùëë ùëí ùëì \n",
    "ùëî ‚Ñé ùëñ)</pre>\n",
    "- The determinant is calculated as:<pre>\n",
    "det(ùê¥)=ùëé(ùëíùëñ‚àíùëì‚Ñé)‚àíùëè(ùëëùëñ‚àíùëìùëî)+ùëê(ùëë‚Ñé‚àíùëíùëî)</pre>\n",
    "\n",
    "![ALT TEXT](img/deter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a matrix with a zero determinant does not have an inverse, making it singular.\n",
    "\n",
    "![ALT TEXT](img/deter0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A negative determinant indicates that the matrix transformation involves a reflection.\n",
    "\n",
    "![ALT TEXT](img/neg_deter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Eigen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Eigen Vectors and Eigen Values (Geometric View)**:\n",
    "- Eigenvectors are special vectors associated with a matrix transformation. When a matrix ùê¥ acts on an eigenvector ùë£, the vector is only scaled and not rotated or reflected.\n",
    "Mathematically, for matrix ùê¥ and eigenvector ùë£, the equation is:\n",
    "ùê¥ùë£=ùúÜùë£\n",
    "\n",
    "Where ùúÜ is the eigenvalue corresponding to the eigenvector ùë£.\n",
    "- **Geometric View**: Eigenvectors point in directions that are **invariant** under the matrix transformation, while eigenvalues describe how much the vector is stretched or compressed.\n",
    "\n",
    "    - If ùúÜ>1, the vector is **stretched**.\n",
    "    - If 0<ùúÜ<1, the vector is **shrunk**.\n",
    "    - If ùúÜ=1, the vector's length remains the **same**.\n",
    "    - If ùúÜ<0, the vector is scaled and **flipped**.\n",
    "\n",
    "![ALT TEXT](img/eigen_geo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Computing Eigenvectors and Eigenvalues:\n",
    "- To find the **eigenvalues** of a matrix ùê¥, solve the characteristic equation:<pre>\n",
    "det(ùê¥‚àíùúÜùêº)=0</pre>\n",
    "\n",
    "Where **ùêº** is the identity matrix and **ùúÜ** represents the eigenvalues.\n",
    "\n",
    "- Once the eigenvalues ùúÜ are found, substitute them into the equation:<pre>\n",
    "(ùê¥‚àíùúÜùêº)ùë£=0</pre>\n",
    "to solve for the **eigenvectors ùë£**. This system of linear equations determines the eigenvectors corresponding to each eigenvalue.\n",
    "\n",
    "Eigenvectors and eigenvalues have many applications in machine learning and data science, particularly in **dimensionality reduction** techniques like PCA (Principal Component Analysis).\n",
    "\n",
    "![ALT TEXT](img/eigen_0.png) ![ALT TEXT](img/eigen_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Geometric View\n",
    "\n",
    "![ALT TEXT](img/eigen_2.png) ![ALT TEXT](img/eigen_3.png) ![ALT TEXT](img/eigen_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Eigen Decomposition**:\n",
    "Eigen decomposition is the process of breaking down a square matrix into its eigenvalues and eigenvectors. It allows us to express the matrix in a form that reveals its geometric and algebraic properties.\n",
    "\n",
    "- Mathematical Formulation:\n",
    "For a square matrix \n",
    "ùê¥ of size ùëõ√óùëõ, if there exists a diagonal **matrix ùê∑** containing the **eigenvalues ùúÜ1,ùúÜ2,...,ùúÜùëõ** and a **matrix ùëâ** consisting of the corresponding **eigenvectors ùë£1,ùë£2,...,ùë£ùëõ**, we can express the eigen decomposition as:<pre>\n",
    "ùê¥=ùëâùê∑ùëâ‚àí1</pre>\n",
    "\n",
    "Where:\n",
    "- **ùê¥** is the original matrix.\n",
    "- **ùëâ** is the matrix of eigenvectors, where each column is an eigenvector corresponding to an eigenvalue.\n",
    "- **ùê∑** is a diagonal matrix where each diagonal entry is an eigenvalue.\n",
    "- **ùëâ‚àí1** is the inverse of the matrix of eigenvectors.\n",
    "\n",
    "![ALT TEXT](img/dec_0.png)\n",
    "![ALT TEXT](img/dec_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Geometric overview\n",
    "\n",
    "![ALT TEXT](img/dec_geo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. The Spectral Theorem**:\n",
    "![ALT TEXT](img/spect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Geometric view\n",
    "\n",
    "![ALT TEXT](img/spect_geo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Matrix Inverse from eigen decomposition\n",
    "\n",
    "![ALT TEXT](img/inv_spect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. What is SVD**\n",
    "Singular Value Decomposition (SVD) is a method of decomposing a matrix into three other matrices. For a given matrix ùê¥ of dimensions \n",
    "ùëö√óùëõ, SVD can be expressed as:\n",
    "ùê¥=ùëà.ùê∑.ùëâùëá \n",
    "Where:\n",
    "- **ùëà** is an ùëö√óùëö orthogonal matrix whose columns are the left singular vectors of ùê¥.\n",
    "- **ùê∑** is an ùëö√óùëõ diagonal matrix containing the singular values of ùê¥ (non-negative and sorted in descending order).\n",
    "- **ùëâ** is an ùëõ√óùëõ orthogonal matrix whose columns are the right singular vectors of A.\n",
    "\n",
    "![ALT TEXT](img/svd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Relationship between Eigen Decomposition and SVD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](img/svd_1.png)\n",
    "![ALT TEXT](img/svd_2.png)\n",
    "![ALT TEXT](img/svd_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Dimentionality Reduction with SVD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALT TEXT](img/dim_red.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
