{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Apache Arrow and Apache Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Apache Arrow?\n",
    "Apache Arrow is a **cross-language development platform** for **in-memory data**. It provides a standard for **columnar data representation** that enables **high-performance analytics** and **data processing**. Arrow is designed to optimize the data layout in-memory, which makes operations like data transfers between systems **very fast and efficient**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key benefits of Arrow:\n",
    "- **Zero-copy data sharing** between different applications and processes.\n",
    "- **Efficient memory usage** by representing data in a columnar format.\n",
    "- **Interoperability** with other data processing systems like Pandas, NumPy, and Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Apache Parquet?\n",
    "Apache Parquet is a columnar storage file format optimized for **efficient reading and writing of data**. It is particularly useful in the **big data ecosystem** where you work with **distributed systems** like Hadoop, Spark, and AWS S3. Parquet is **highly efficient for both storage and query performance**, especially for analytics workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key benefits of Parquet:\n",
    "- **Columnar format**, which provides efficient compression and encoding.\n",
    "- **Schema evolution**, meaning the schema can adapt over time without breaking compatibility.\n",
    "- **Highly optimized** for read-heavy operations typical in big data environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use Arrow and Parquet over Pandas and CSV?\n",
    "Pandas and CSV are often inefficient for handling large datasets due to:\n",
    "- **Memory overhead**: Pandas loads entire datasets into memory, which is a major limitation when dealing with big data.\n",
    "- **Processing bottlenecks**: CSV is a row-based format, which can slow down data access, especially for analytics workflows.\n",
    "\n",
    "In contrast:\n",
    "- **Arrow** allows you to perform efficient, in-memory operations.\n",
    "- **Parquet** provides compressed, efficient, and schema-based storage for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Key Concepts of Apache Arrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Memory Format and Zero-Copy Sharing\n",
    "Arrow stores data in a columnar format that allows for efficient memory layout. This enables \"zero-copy\" reads and writes, which dramatically speeds up data transfers and eliminates the need for data serialization between systems.\n",
    "\n",
    "**For example**, you can transfer data from Arrow to Pandas or NumPy without making a deep copy, saving time and memory.\n",
    "\n",
    "## Arrow Table and Schema\n",
    "An Arrow Table is a data structure that represents a table of data with a schema. Each column in the table is a contiguous block of memory. Arrow Schemas define the data types and structure of your data.\n",
    "\n",
    "## Interoperability with Pandas\n",
    "Arrow seamlessly integrates with Pandas, enabling conversion between Pandas DataFrames and Arrow Tables. This allows you to maintain familiarity with Pandas while leveraging Arrow for performance benefits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Key Concepts of Apache Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columnar Storage\n",
    "Parquet stores data column by column, which is different from row-based formats like CSV. This format is ideal for analytical queries that involve selecting specific columns, as it minimizes disk I/O by reading only the necessary data.\n",
    "\n",
    "## Compression and Encoding\n",
    "Parquet supports various compression methods (e.g., Snappy, Gzip) and encoding schemes (e.g., dictionary encoding, run-length encoding). This can greatly reduce file sizes and speed up reading operations.\n",
    "\n",
    "## Schema Evolution\n",
    "Parquet allows you to evolve your data schema over time. You can add new columns or modify existing ones without breaking existing Parquet files. This is a critical feature for data pipelines that need to handle changing data requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Installing Arrow and Parquet Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Apache Arrow and Apache Parquet are available through the **pyarrow library in Python**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "<pre>\n",
    "pip install pyarrow</pre>\n",
    "This library allows you to work with both Arrow and Parquet file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Using Apache Arrow with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Arrow Tables\n",
    "Here’s how you can create an Arrow Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "column1: int64\n",
      "column2: string\n",
      "----\n",
      "column1: [[1,2,3]]\n",
      "column2: [[\"a\",\"b\",\"c\"]]\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "data = {\n",
    "    'column1': [1, 2, 3],\n",
    "    'column2': ['a', 'b', 'c']\n",
    "}\n",
    "\n",
    "table = pa.Table.from_pydict(data)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates an Arrow Table from a Python dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing Arrow Files\n",
    "You can save Arrow data to disk and read it back:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feather (.feather)** is fast and simple for smaller datasets.\n",
    "- **Arrow IPC (.arrow)** is more flexible and can handle larger, partitioned datasets, making it ideal for distributed processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s how you can save and load data using the **.feather** format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.feather as feather\n",
    "\n",
    "# Write to Arrow (Feather format)\n",
    "feather.write_feather(table, 'data.feather')\n",
    "\n",
    "# Read from Arrow (Feather format)\n",
    "table = feather.read_feather('data.feather')\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s how you can save and load data in **multiple files** using the **.arrow** format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.ipc as ipc\n",
    "\n",
    "# Create an Arrow Table\n",
    "data = {'column1': [1, 2, 3], 'column2': ['a', 'b', 'c']}\n",
    "table = pa.Table.from_pydict(data)\n",
    "\n",
    "# Split the table into chunks (for example, 2 files with 50 rows each)\n",
    "batch_size = 50\n",
    "batches = [table.slice(offset, batch_size) for offset in range(0, len(table), batch_size)]\n",
    "\n",
    "# Write each chunk to a separate .arrow file\n",
    "for i, batch in enumerate(batches):\n",
    "    filename = f\"data-{i:05d}-of-{len(batches):05d}.arrow\"\n",
    "    with pa.OSFile(filename, 'wb') as sink:\n",
    "        writer = ipc.RecordBatchFileWriter(sink, batch.schema)\n",
    "        writer.write_table(batch)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "\n",
    "tables = []\n",
    "num_files = 2  # The number of files we saved earlier\n",
    "\n",
    "for i in range(num_files):\n",
    "    filename = f\"data-{i:05d}-of-{num_files:05d}.arrow\"\n",
    "    with pa.memory_map(filename, 'r') as source:\n",
    "        reader = ipc.RecordBatchFileReader(source)\n",
    "        table = reader.read_all()\n",
    "        tables.append(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting between Arrow and Pandas\n",
    "Arrow integrates smoothly with Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert Pandas DataFrame to Arrow Table\n",
    "df = pd.DataFrame({'column1': [1, 2, 3], 'column2': ['a', 'b', 'c']})\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Convert Arrow Table back to Pandas DataFrame\n",
    "df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing and Modifying Data in Arrow Tables\n",
    "Arrow Tables are **immutable**, meaning once they are created, you cannot modify them directly. However, you can create new tables by transforming the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing data in columns\n",
    "You can access individual columns and data using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a column by name\n",
    "col1 = table.column('column1')\n",
    "print(col1)\n",
    "\n",
    "# Access data in a specific column\n",
    "col1_data = col1.to_pylist()  # Convert to a list\n",
    "print(col1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying data in columns\n",
    "Although Arrow tables are immutable, you can create a new table with the desired modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace a column\n",
    "new_col = pa.array([10, 20, 30])\n",
    "new_table = table.set_column(0, 'column1', new_col)  # Replace column at index 0\n",
    "print(new_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add new columns or remove existing ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column\n",
    "new_col = pa.array([100, 200, 300])\n",
    "table_with_new_col = table.append_column('column3', new_col)\n",
    "\n",
    "# Remove a column\n",
    "table_without_col2 = table.drop(['column2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Operations\n",
    "Arrow provides fast and efficient operations on columns. You can perform basic arithmetic and logical operations across entire columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform column arithmetic\n",
    "col1 = table.column('column1').cast(pa.int64())  # Convert to a numeric type if needed\n",
    "col_sum = col1.add(pa.array([10, 10, 10]))  # Add 10 to each value in column1\n",
    "print(col_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = table.column('column1').cast(pa.int64())  # Convert to a numeric type if needed\n",
    "col_sum = col1.add(pa.array([10, 10, 10]))  # Add 10 to each value in column1\n",
    "print(col_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element-Wise Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Handling Missing Data\n",
    "Arrow supports missing data using null values. You can explicitly create arrays with missing data or detect and handle nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array with null values\n",
    "arr_with_nulls = pa.array([1, None, 3], type=pa.int64())\n",
    "\n",
    "# Count the number of nulls\n",
    "num_nulls = arr_with_nulls.null_count\n",
    "print(f\"Number of nulls: {num_nulls}\")\n",
    "\n",
    "# Fill nulls with a default value\n",
    "filled_array = arr_with_nulls.fill_null(0)\n",
    "print(filled_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Computing Statistics on Arrow Data\n",
    "Apache Arrow provides several functions to compute statistics on columns. You can compute things like sums, means, and even aggregate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.compute as pc\n",
    "\n",
    "# Compute sum, mean, min, and max\n",
    "sum_col1 = pc.sum(table.column('column1'))\n",
    "mean_col1 = pc.mean(table.column('column1'))\n",
    "min_col1 = pc.min(table.column('column1'))\n",
    "max_col1 = pc.max(table.column('column1'))\n",
    "\n",
    "print(f\"Sum: {sum_col1.as_py()}, Mean: {mean_col1.as_py()}, Min: {min_col1.as_py()}, Max: {max_col1.as_py()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations are **vectorized**, meaning they are very fast and efficient for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating and Slicing Tables\n",
    "You can concatenate multiple Arrow Tables or slice tables to get subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = pa.Table.from_pydict({'column1': [4, 5, 6], 'column2': ['d', 'e', 'f']})\n",
    "\n",
    "# Concatenate two tables\n",
    "combined_table = pa.concat_tables([table, table2])\n",
    "print(combined_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the table to get the first two rows\n",
    "sliced_table = table.slice(0, 2)\n",
    "print(sliced_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining and Merging Tables\n",
    "Although Apache Arrow doesn’t have native join operations like SQL databases or Pandas, you can convert Arrow Tables to Pandas DataFrames for complex join operations, then convert them back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Using Apache Parquet with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Parquet Files\n",
    "You can write a Pandas DataFrame or an Arrow Table to a Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Write Arrow Table to Parquet\n",
    "pq.write_table(table, 'data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Parquet Files\n",
    "You can read a Parquet file into an Arrow Table or a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet file into Arrow Table\n",
    "table = pq.read_table('data.parquet')\n",
    "print(table)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a **Partitioned Parquet Dataset**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Write the table to a partitioned Parquet dataset, partitioning by 'column1'\n",
    "pq.write_to_dataset(table, root_path='output_parquet', partition_cols=['column1'])\n",
    "\n",
    "print(\"Written partitioned Parquet dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will create a directory output_parquet with subdirectories like column1=1/, column1=2/, and will place Parquet files in each subdirectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a **Partitioned Parquet Dataset**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the partitioned Parquet dataset\n",
    "dataset = pq.ParquetDataset('output_parquet')\n",
    "table = dataset.read()\n",
    "\n",
    "# Convert back to Pandas DataFrame for easy manipulation\n",
    "df = table.to_pandas()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Large Datasets in Parquet\n",
    "Parquet’s columnar format and compression make it highly efficient for working with large datasets. You can use it for large-scale analytics without loading all the data into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Combining Apache Arrow and Apache Parquet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Parquet into Arrow\n",
    "Parquet and Arrow are highly compatible, so you can easily read a Parquet file into Arrow for further processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pq.read_table('data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Arrow Data to Parquet\n",
    "Likewise, you can convert an Arrow Table into Parquet for optimized storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(table, 'output.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarks and Case Studies\n",
    "When dealing with large datasets, you’ll notice significant improvements in performance and memory usage compared to Pandas and CSV:\n",
    "\n",
    "Faster I/O: Loading data from Parquet into Arrow is much faster.\n",
    "Lower Memory Usage: Both Arrow and Parquet are optimized for in-memory processing and storage efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Advantages and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization\n",
    "To further optimize performance, you can:\n",
    "- Use **multi-threaded I/O** to read and write Parquet files faster.\n",
    "- Leverage **Arrow’s zero-copy** sharing across applications for fast data access.\n",
    "## Memory Efficiency\n",
    "Arrow is designed for memory efficiency, so it avoids creating unnecessary copies of data. Combined with Parquet’s compression, you can process larger datasets with lower memory footprints.\n",
    "\n",
    "## Use Cases for Big Data Workflows\n",
    "- **Data Analytics**: Columnar formats are ideal for running queries on large datasets.\n",
    "- **Data Engineering**: Parquet and Arrow can efficiently handle ETL (Extract, Transform, Load) workflows.\n",
    "- **Machine Learning**: Load training data quickly and efficiently without overwhelming memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
