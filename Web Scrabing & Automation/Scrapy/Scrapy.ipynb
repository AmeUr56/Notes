{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is Scrapy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy is an open-source Python framework designed for **extracting data from websites** (web scraping). It can also be used for other types of web crawling, like gathering data for analytics or indexing websites.\n",
    "\n",
    "## Why Use Scrapy?\n",
    "- **Fast**: Scrapy uses asynchronous requests, which makes it faster compared to other scraping libraries like BeautifulSoup.\n",
    "- **Built-in Tools**: Scrapy offers tools for handling requests, following links, handling pagination, and managing data pipelines.\n",
    "- **Handling Complex Websites**: With Scrapy, you can deal with dynamic content, form submissions, cookies, and authentication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Scrapy using pip:<pre>\n",
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setting Up a Scrapy Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 How to Create a Scrapy Project\n",
    "To create a new Scrapy project:\n",
    "- Open a terminal or command prompt.\n",
    "- Navigate to the directory where you want to create the project.\n",
    "- Run the following command:<pre>\n",
    "scrapy startproject myproject</pre>\n",
    "This will create a new Scrapy project in a directory called myproject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Overview of the Scrapy Project\n",
    "The project created by Scrapy will have the following structure:<pre>\n",
    "_ myproject/\n",
    "_   scrapy.cfg             # Configuration file for the project\n",
    "_   myproject/             # Python module for the project\n",
    "_       __init__.py        # Makes 'myproject' a Python package\n",
    "_       items.py           # Defines item classes\n",
    "_       middlewares.py     # For custom middlewares\n",
    "_       pipelines.py       # For item processing pipelines\n",
    "_       settings.py        # Project settings\n",
    "_       spiders/           # Store spider classes (core scraping logic)\n",
    "_           __init__.py    # Makes 'spiders' a Python package</pre><br>\n",
    "**scrapy.cfg**: This is the project configuration file, which links the project's settings to the Scrapy tool.<br>\n",
    "**items.py**: Defines the structure of the data you want to scrape (like a schema).<br>\n",
    "**middlewares.py**: Contains custom middlewares that modify requests/responses.<br>\n",
    "**pipelines.py**: Defines how the scraped data is processed or stored after being scraped.<br>\n",
    "**settings.py**: This file contains all the configuration settings for the Scrapy project (such as enabling/disabling features, customizing behavior).<br>\n",
    "**spiders/**: This folder stores all the spiders (the actual scraping scripts) you will write.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Explaining Scrapy Spiders, Items & Item Pipelines\n",
    "### **Spiders**:\n",
    "A spider is a class responsible for defining how Scrapy will navigate through a website and extract the information you need.\n",
    "Spiders are placed inside the spiders/ directory.\n",
    "A basic spider looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = \"example\"\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        yield {\n",
    "            'title': response.css('title::text').get(),\n",
    "            'url': response.url\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**start_urls**: The initial URL(s) the spider will start scraping.<br>\n",
    "**parse method**: Defines how to extract and return the data from the page using Scrapy selectors (.css or .xpath).\n",
    "\n",
    "### **Items**:\n",
    "Items are used to define the data structure of what you’re scraping.\n",
    "You create an item by defining it in items.py like this:<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class MyItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    url = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then yield MyItem in the spider instead of returning a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myproject.items import MyItem\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "   name = \"example\"\n",
    "   start_urls = ['http://example.com']\n",
    "   def parse(self, response):\n",
    "       item = MyItem()\n",
    "       item['title'] = response.css('title::text').get()\n",
    "       item['url'] = response.url\n",
    "       yield item\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Item Pipelines**:\n",
    "Pipelines are used for post-processing the scraped items (e.g., cleaning the data, saving it to a database, etc.).\n",
    "Pipelines are defined in pipelines.py. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPipeline:\n",
    "   def process_item(self, item, spider):\n",
    "       # Process item (e.g., clean data, save to DB)\n",
    "       return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to enable the pipeline in settings.py by adding it to ITEM_PIPELINES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {\n",
    "    'myproject.pipelines.MyPipeline': 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The number 300 determines the order in which pipelines run (lower numbers run first)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Explaining Scrapy Middlewares & Settings\n",
    "### **Middlewares**:\n",
    "\n",
    "Middlewares are hooks that allow you to modify Scrapy's requests and responses during the scraping process. They act between the engine, spiders, and the web.<br>\n",
    "Middlewares can be customized in middlewares.py.\n",
    "Examples of middlewares:\n",
    "- **Downloader Middleware**: Modifies requests or responses before they reach spiders (e.g., setting custom headers, retrying failed requests).\n",
    "- **Spider Middleware**: Modifies the spider’s output before it's processed by the Scrapy engine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomMiddleware:\n",
    "    def process_request(self, request, spider):\n",
    "        # Modify request (e.g., set user agent)\n",
    "        request.headers['User-Agent'] = 'my-custom-user-agent'\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You enable the middleware in settings.py:<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'myproject.middlewares.MyCustomMiddleware': 543,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Settings**:\n",
    "Scrapy's behavior can be customized using the settings.py file.\n",
    "#### Common settings include:\n",
    "- **USER_AGENT**: Change the default user agent.\n",
    "- **DOWNLOAD_DELAY**: Delay between requests to avoid being blocked.\n",
    "- **ROBOTSTXT_OBEY**: Whether Scrapy should respect robots.txt rules.\n",
    "- **CONCURRENT_REQUESTS**: The number of simultaneous requests Scrapy can send.\n",
    "\n",
    "Example configuration in settings.py:<pre>\n",
    "USER_AGENT = 'my-scrapy-bot'\n",
    "DOWNLOAD_DELAY = 2  # Wait 2 seconds between requests\n",
    "ROBOTSTXT_OBEY = True  # Respect robots.txt rules\n",
    "CONCURRENT_REQUESTS = 8  # Max number of concurrent requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Crawl inside Python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from quotes.spiders.quotes_spider import QuotesSpider\n",
    "\n",
    "def run_spider():\n",
    "    process = CrawlerProcess()\n",
    "    process.crawl(QuotesSpider)\n",
    "    process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build your First Scrapy Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 How to Create a Scrapy Spider\n",
    "Creating a Scrapy spider is easy using the genspider command. This command sets up the spider for you without needing to manually create a file.<br>\n",
    "Navigate to your Scrapy project directory in your terminal:<pre>\n",
    "cd path/to/your/project</pre>\n",
    "Create a new spider using the genspider command:<pre>\n",
    "scrapy genspider quotes http://quotes.toscrape.com</pre>\n",
    "**quotes**: This is the name of the spider.<br>\n",
    "**http://quotes.toscrape.com**: The starting URL that the spider will scrape.<br>\n",
    "This command creates a new spider file under the spiders/ directory. The file will look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "   name = 'quotes'\n",
    "   allowed_domains = ['quotes.toscrape.com']\n",
    "   start_urls = ['http://quotes.toscrape.com/']\n",
    "   def parse(self, response):\n",
    "       pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the parse method to define what data you want to scrape and how to extract it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    for quote in response.css('div.quote'):\n",
    "        yield {\n",
    "            'text': quote.css('span.text::text').get(),\n",
    "            'author': quote.css('span small::text').get(),\n",
    "            'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**name**: The name of the spider (used to run it).<br>\n",
    "**allowed_domains**: Restricts the spider to this domain.<br>\n",
    "**start_urls**: The URLs the spider starts scraping from.<br>\n",
    "**parse()**: This method processes the response, extracts the data, and yields it.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Using Scrapy Shell to Find CSS Selectors\n",
    "The Scrapy shell is a great tool for experimenting with extracting data using CSS or XPath selectors before implementing them in your spider. Below are some useful and common commands you can run inside the Scrapy shell.<br>\n",
    "Open the Scrapy shell by running the following command in your terminal:<pre>\n",
    "scrapy shell 'http://quotes.toscrape.com/page/1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Scrapy Shell Commands:\n",
    "- **View the page's HTML**:<pre>\n",
    "view(response)</pre>\n",
    "\n",
    "This command opens the current HTML page in your browser so you can inspect it.\n",
    "\n",
    "- **Extract all elements matching a CSS selector**:<pre>\n",
    "response.css('div.quote')</pre>\n",
    "\n",
    "Returns all elements that match the div.quote selector.\n",
    "\n",
    "- **Extract text from a CSS selector**:<pre>\n",
    "response.css('span.text::text').getall()</pre>\n",
    "\n",
    "Extracts the text from all span.text elements.\n",
    "\n",
    "- **Extract a single result**:<pre>\n",
    "response.css('span.text::text').get()<pre>\n",
    "\n",
    "Extracts only the first matching text element.\n",
    "\n",
    "- **Extract attributes (e.g., URLs)**:<pre>\n",
    "response.css('a::attr(href)').getall()<pre>\n",
    "\n",
    "Extracts all href attributes from a (anchor) tags.\n",
    "\n",
    "- **XPath selectors (an alternative to CSS selectors)**:<pre>\n",
    "response.xpath('//div[@class=\"quote\"]/span[@class=\"text\"]/text()').getall()<pre>\n",
    "\n",
    "Extracts all quote text elements using XPath.\n",
    "\n",
    "- **Extracting an element by its ID**:<pre>\n",
    "response.css('#unique-id::text').get()<pre>\n",
    "\n",
    "Extracts the text from an element with the ID unique-id.\n",
    "\n",
    "- **Check response status**:<pre>\n",
    "response.status<pre>\n",
    "\n",
    "Checks the HTTP status of the response (e.g., 200 OK).\n",
    "\n",
    "- **Extracting JSON response (if scraping an API)**:<pre>\n",
    "response.json()<pre>\n",
    "\n",
    "Converts the response to JSON format if the page returns JSON data.\n",
    "\n",
    "- **Following links**:<pre>\n",
    "response.follow(response.css('a::attr(href)').get(), callback=self.parse)<pre>\n",
    "\n",
    "Follows the first link found and continues parsing.\n",
    "\n",
    "- **Getting the full URL**:<pre>\n",
    "response.url<pre>\n",
    "\n",
    "Returns the URL of the current page.\n",
    "\n",
    "- **Closing the shell**:<pre>\n",
    "exit()<pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Using CSS Selectors in Our Spider to Get Data\n",
    "Once you have tested your selectors in the Scrapy shell, you can now integrate them into your spider's parse method. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    for quote in response.css('div.quote'):\n",
    "        yield {\n",
    "            'text': quote.css('span.text::text').get(),\n",
    "            'author': quote.css('span small::text').get(),\n",
    "            'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will extract the quote text, author, and tags from each quote on the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Getting Our Spider to Navigate to Multiple Pages\n",
    "To scrape multiple pages, modify the spider to follow the \"Next\" page link.<br>\n",
    "\n",
    "In the parse method, find the \"Next\" page link and follow it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    for quote in response.css('div.quote'):\n",
    "        yield {\n",
    "            'text': quote.css('span.text::text').get(),\n",
    "            'author': quote.css('span small::text').get(),\n",
    "            'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "        }\n",
    "\n",
    "    # Get the link to the next page and follow it if it exists\n",
    "    next_page = response.css('li.next a::attr(href)').get()\n",
    "    if next_page is not None:\n",
    "        next_page = response.urljoin(next_page)\n",
    "        yield scrapy.Request(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This spider will now scrape all pages by following the \"Next\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Build Discovery & Extraction Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 How to Crawl Pages with Scrapy Spiders\n",
    "Scrapy spiders can crawl multiple pages by either following links on each page or defining a set of URLs to visit.<br>\n",
    "Define Starting URLs: In your spider, list the URLs where you want to start the crawl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class DiscoverySpider(scrapy.Spider):\n",
    "   name = 'discovery'\n",
    "   start_urls = ['http://quotes.toscrape.com/']\n",
    "   def parse(self, response):\n",
    "       # This method will handle the response and extract data from it\n",
    "       pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following Links: \n",
    "You can follow links to crawl through multiple pages. For example, to follow the \"Next\" page link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    for quote in response.css('div.quote'):\n",
    "        yield {\n",
    "            'text': quote.css('span.text::text').get(),\n",
    "            'author': quote.css('small::text').get(),\n",
    "            'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "        }\n",
    "    # Follow the \"Next\" page link\n",
    "    next_page = response.css('li.next a::attr(href)').get()\n",
    "    if next_page is not None:\n",
    "        yield response.follow(next_page, callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This spider starts at the first page, extracts data, and then follows the \"Next\" page link until there are no more pages to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Using XPath Queries to Extract Data\n",
    "XPath (XML Path Language) is a powerful query language used to select nodes from an XML document or HTML page. It allows you to navigate through elements and attributes, making it extremely useful for web scraping when HTML structures get complex.\n",
    "\n",
    "Here’s a detailed breakdown of common and useful XPath queries in Scrapy:\n",
    "\n",
    "### 1. Basic XPath Syntax\n",
    "XPath is written as a path-like structure. Here are some fundamental concepts:\n",
    "\n",
    "- **Selecting Nodes**:\n",
    "/ selects from the root node.\n",
    "// selects nodes anywhere in the document.\n",
    ".// selects nodes relative to the current node.\n",
    "Example:<pre>\n",
    "//div[@class=\"quote\"]  # Select all \\<div> elements with class=\"quote\"</pre>\n",
    "### 2. Selecting Elements\n",
    "- **Select all elements of a specific type**:<pre>\n",
    "//a      # Select all \\<a> elements</pre>\n",
    "- **Select an element by attribute**:<pre>\n",
    "//div[@id=\"content\"]  # Select a \\<div> element with id=\"content\"</pre>\n",
    "- **Select multiple attributes**:<pre>\n",
    "//div[@class=\"quote\" and @id=\"unique\"]  # Select a \\<div> with both class and id</pre>\n",
    "### 3. Selecting Text Content\n",
    "- **Select text within an element**:<pre>\n",
    "//span[@class=\"text\"]/text()  # Extract text from \\<span> with class=\"text\"</pre>\n",
    "- **Extract text even when it's nested**:<pre>\n",
    "//div[@class=\"quote\"]//text()  # Get all text inside \\<div class=\"quote\"></pre>\n",
    "- **Select normalized text (no extra spaces)**:<pre>\n",
    "normalize-space(//span[@class=\"text\"])  # Strip spaces around the text</pre>\n",
    "### 4. Selecting Attributes\n",
    "- **Extract an attribute's value**:<pre>\n",
    "//a/@href   # Extract the 'href' attribute from \\<a> elements</pre>\n",
    "- **Extract multiple attributes**:<pre>\n",
    "//img/@src | //img/@alt   # Extract both src and alt attributes from \\<img></pre>\n",
    "### 5. Using Wildcards\n",
    "- **Select elements regardless of tag type**:<pre>\n",
    "//*[@class=\"quote\"]   # Select any element with class=\"quote\"</pre>\n",
    "- **Wildcard for partial attribute matching**:\n",
    "    - **contains()** function checks for partial matches:<pre>\n",
    "//a[contains(@href, 'page')]   # Select all \\<a> tags with 'page' in href attribute\n",
    "- **Using * to match any element type**:<pre>\n",
    "//div/*    # Select all child elements of \\<div></pre>\n",
    "### 6. Traversing Nodes\n",
    "- **Select direct child nodes**:<pre>\n",
    "//div[@class=\"quote\"]/span    # Select only \\<span> elements directly under \\<div></pre>\n",
    "- **Select all descendant nodes (direct + indirect children)**:<pre>\n",
    "//div[@class=\"quote\"]//span   # Select all \\<span> elements under \\<div>, regardless of depth</pre>\n",
    "- **Select parent nodes**:<pre>\n",
    "//span[@class=\"text\"]/..      # Select the parent of \\<span> with class=\"text\"</pre>\n",
    "- **Select sibling nodes**:<pre>\n",
    "//h2[@class=\"heading\"]/following-sibling::p   # Select \\<p> that comes right after \\<h2></per>\n",
    "### 7. Indexing and Positional Selection\n",
    "- **Select the first/last element**:<pre>\n",
    "//div[@class=\"quote\"][1]     # Select the first \\<div> with class=\"quote\"\n",
    "//div[@class=\"quote\"][last()]   # Select the last \\<div> with class=\"quote\"</pre>\n",
    "- **Select a specific element by position**:<pre>\n",
    "//div[@class=\"quote\"][position()=3]   # Select the third \\<div> with class=\"quote\"</pre>\n",
    "- **Select all elements after a specific one**:<pre>\n",
    "//div[@class=\"quote\"][position()>2]   # Select all \\<div> after the second one\n",
    "### 8. Conditional Expressions\n",
    "XPath supports conditional logic to refine selections.\n",
    "\n",
    "- **Select elements with specific conditions**:<pre>\n",
    "//div[@class=\"quote\" and contains(., \"life\")]   # Select \\<div> with class=\"quote\" that contains the text \"life\"</pre>\n",
    "- **Combining conditions**:<pre>\n",
    "//div[@class=\"quote\"]//span[contains(text(), 'wisdom') or contains(text(), 'life')]\n",
    "#Select \\<span> inside \\<div class=\"quote\"> where text contains either 'wisdom' or 'life'\n",
    "### 9. Using Functions\n",
    "XPath provides various functions to manipulate and extract data.\n",
    "\n",
    "- **contains()**: Matches elements or attributes containing a certain substring.<pre>\n",
    "//a[contains(@href, 'example.com')]   # Select links that contain 'example.com'</pre>\n",
    "- **starts-with()**: Matches elements or attributes that start with a certain string.<pre>\n",
    "//a[starts-with(@href, 'http')]   # Select links that start with 'http'</pre>\n",
    "- **text()** and **normalize-space()**:<pre>\n",
    "normalize-space(//span[@class=\"text\"])   # Remove leading/trailing whitespace from text</pre>\n",
    "- **Arithmetic operations**:\n",
    "//li[position() mod 2 = 0]   # Select all even-numbered <li> elements\n",
    "### 10. Scrapy-Specific Usage of XPath\n",
    "In Scrapy, XPath queries are used within the spider's **parse()** method to extract data:\n",
    "\n",
    "- **Basic example**:<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    quotes = response.xpath('//div[@class=\"quote\"]')\n",
    "    for quote in quotes:\n",
    "        yield {\n",
    "            'text': quote.xpath('span[@class=\"text\"]/text()').get(),\n",
    "            'author': quote.xpath('span/small/text()').get(),\n",
    "            'tags': quote.xpath('div[@class=\"tags\"]/a[@class=\"tag\"]/text()').getall(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Saving the Data to CSV or JSON Format\n",
    "Once you’ve crawled and extracted your data, Scrapy can automatically save it to CSV, JSON, or other formats. This is done using the command line when running your spider.<br>\n",
    "\n",
    "- **Save Data as JSON**:\n",
    "To run your spider and save the output as a JSON file, use the following command:<pre>\n",
    "scrapy crawl discovery -o output.json</pre>\n",
    "Scrapy will create a output.json file and save all the scraped data into it.\n",
    "\n",
    "- **Save Data as CSV**:\n",
    "To save your data as a CSV file, run:><pre>\n",
    "scrapy crawl discovery -o output.csv</pre>\n",
    "This will output the data in a CSV format with one line per scraped item.\n",
    "\n",
    "- **Save Data as XML**:\n",
    "To save data as XML, run:<pre>\n",
    "scrapy crawl discovery -o output.xml</pre>\n",
    "Scrapy's output format is determined by the -o flag, allowing you to quickly export your data in any common format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cleaning Data with Item Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy provides a feature called Item Pipelines that allows you to process and clean the data after it's been scraped but before it's stored or exported. Pipelines can be used for tasks such as cleaning data, validating fields, removing duplicates, or saving the data in various formats (e.g., databases, JSON, CSV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 What are Scrapy Items?\n",
    "Scrapy Items are containers for storing the scraped data. They are similar to Python dictionaries but provide additional structure and validation.<br>\n",
    "You define items in the items.py file within your Scrapy project.<br>\n",
    "\n",
    "- **Creating an Item**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class QuoteItem(scrapy.Item):\n",
    "    text = scrapy.Field()\n",
    "    author = scrapy.Field()\n",
    "    tags = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Purpose**:\n",
    "    - Items help ensure that your scraped data is organized and structured.\n",
    "    - They make it easier to apply validation and transformations through pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Using Scrapy Items to Structure Our Data\n",
    "Once you've defined your Item class, you can use it in your spider to structure the data you're scraping.\n",
    "\n",
    "- **Example of Using Items in Spider**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myproject.items import QuoteItem\n",
    "\n",
    "def parse(self, response):\n",
    "\n",
    "    quotes = response.xpath('//div[@class=\"quote\"]')\n",
    "\n",
    "    for quote in quotes:\n",
    "\n",
    "       item = QuoteItem()\n",
    "\n",
    "       item['text'] = quote.xpath('span[@class=\"text\"]/text()').get()\n",
    "\n",
    "       item['author'] = quote.xpath('span/small/text()').get()\n",
    "\n",
    "       item['tags'] = quote.xpath('div[@class=\"tags\"]/a[@class=\"tag\"]/text()').getall()\n",
    "\n",
    "       yield item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Benefits**:\n",
    "    - Structured data output.\n",
    "    - Easier to work with in pipelines and when saving data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 What are Scrapy Pipelines?\n",
    "Scrapy Pipelines are used to process items after they’ve been scraped but before they’re saved/exported. Each pipeline is a class that processes the data through the **process_item()** method, using ItemAdapter to access fields in a unified way.\n",
    "\n",
    "##### **Key Pipeline Functions**:\n",
    "- **process_item(self, item, spider)**: The main method that processes each scraped item.\n",
    "- **open_spider(self, spider)**: Runs when the spider opens (useful for setup tasks).\n",
    "- **close_spider(self, spider)**: Runs when the spider closes (useful for cleanup tasks).\n",
    "\n",
    "Now, Scrapy uses ItemAdapter to standardize how data is accessed, regardless of whether it's an Item, dictionary, or other data structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Cleaning Our Data with Item Pipelines (Updated Example)\n",
    "#### Example: Cleaning Data in a Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itemadapter import ItemAdapter\n",
    "\n",
    "class CleanQuotePipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        adapter = ItemAdapter(item)\n",
    "\n",
    "            # Clean text\n",
    "        adapter['text'] = adapter['text'].strip().replace('\\n', '')\n",
    "\n",
    "            # Clean author\n",
    "        adapter['author'] = adapter['author'].strip()\n",
    "\n",
    "            # Clean tags (remove empty tags, strip whitespace)\n",
    "        adapter['tags'] = [tag.strip() for tag in adapter['tags'] if tag.strip()]\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "- **ItemAdapter(item)**: This creates a unified interface for accessing the fields in the item, whether it's a Scrapy Item or a plain dictionary.\n",
    "The rest of the code performs basic cleaning: stripping whitespace and filtering empty tags.\n",
    "\n",
    "#### Enabling Pipelines\n",
    "After creating the pipeline, don’t forget to enable it in the **settings.py** file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {\n",
    "    'myproject.pipelines.CleanQuotePipeline': 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Saving Data to Files & Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Saving Data via Command Line\n",
    "Scrapy allows you to save scraped data directly to files (such as JSON, CSV, or XML) from the command line.\n",
    "\n",
    "##### **Basic Command**: You can run the spider and save the output to a file format of your choice using the following command:<pre>\n",
    "scrapy crawl spider_name -o output_file.format</pre>\n",
    "##### **Example**:<pre>\n",
    "scrapy crawl quotes -o quotes.json\n",
    "scrapy crawl quotes -O quotes.csv\n",
    "</pre>\n",
    "\n",
    "- **O** -> Overview.\n",
    "- **o** -> Append.\n",
    "\n",
    "- **Supported Formats**:\n",
    "JSON, JSON Lines (for streaming JSON records), CSV, XML, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Saving Data via Feed Settings\n",
    "Scrapy’s settings allow you to configure output feeds in settings.py, so every time your spider runs, it automatically saves the data to a file.\n",
    "\n",
    "##### **Example**: \n",
    "- In your project’s settings.py, add the following code to save the scraped data to a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEEDS = {\n",
    "  'output.json': {\n",
    "      'format': 'json',\n",
    "      'encoding': 'utf8',\n",
    "      'overwrite': True  # Set this to False to append instead of overwrite\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can save to multiple formats or even multiple files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEEDS = {\n",
    "  'output.json': {\n",
    "      'format': 'json',\n",
    "      'encoding': 'utf8',\n",
    "      'overwrite': True\n",
    "  },\n",
    "  'output.csv': {\n",
    "      'format': 'csv',\n",
    "      'fields': ['field1', 'field2'],  # Optional: Select specific fields\n",
    "  },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Advantages**:\n",
    "- This method is convenient when running spiders regularly.\n",
    "- You can control the format, encoding, and more through configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Saving Data Into Databases\n",
    "You can also save the scraped data directly into databases such as MySQL, PostgreSQL, or SQLite using pipelines.\n",
    "\n",
    "#### Setting Up Database Connection\n",
    "- **Install Database Drivers**: Depending on the database you're using, you need the appropriate driver:\n",
    "    - For MySQL: pip install pymysql\n",
    "    - For PostgreSQL: pip install psycopg2\n",
    "    - For SQLite: No need to install anything extra (SQLite is part of Python's standard library).\n",
    "- **Writing a Database Pipeline**: Here’s an example of how to insert data into a SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "class SQLitePipeline:\n",
    "    def open_spider(self, spider):\n",
    "        self.connection = sqlite3.connect('quotes.db')\n",
    "        self.cursor = self.connection.cursor()\n",
    "        self.cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS quotes (\n",
    "                text TEXT,\n",
    "                author TEXT,\n",
    "                tags TEXT\n",
    "            )\n",
    "        ''')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.connection.commit()\n",
    "        self.connection.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        adapter = ItemAdapter(item)\n",
    "        self.cursor.execute('''\n",
    "            INSERT INTO quotes (text, author, tags) VALUES (?, ?, ?)\n",
    "        ''', (\n",
    "            adapter.get('text'),\n",
    "            adapter.get('author'),\n",
    "            ','.join(adapter.get('tags', []))  # Convert list of tags to a comma-separated string\n",
    "        ))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**:\n",
    "    - **open_spider()**: Opens the database connection and creates the quotes table if it doesn't exist.\n",
    "    - **close_spider()**: Commits any remaining transactions and closes the connection when the spider finishes.\n",
    "    - **process_item()**: Inserts each scraped item into the quotes table.\n",
    "\n",
    "- **Enabling the Database Pipeline**<br>\n",
    "Activate the database pipeline in settings.py by adding it to the ITEM_PIPELINES section:<pre>\n",
    "_ ITEM_PIPELINES = {\n",
    "_   'myproject.pipelines.SQLitePipeline': 300,\n",
    "_ }\n",
    "- **The number in ITEM_PIPELINES**<br>\n",
    "The number in the ITEM_PIPELINES setting represents the order in which the pipelines are executed. Lower numbers are executed first, and higher numbers are executed later,example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {\n",
    "   'myproject.pipelines.CleaningPipeline': 200,  # Executes first\n",
    "   'myproject.pipelines.SQLitePipeline': 300,    # Executes second\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case:\n",
    "- CleaningPipeline with priority 200 will run first, to clean or modify the scraped data.\n",
    "- SQLitePipeline with priority 300 will run next, saving the cleaned data to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Fake User-Agents & Browser Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When web scraping, it's common to encounter restrictions, blocks, or CAPTCHAs because websites detect and block automated scripts. In this section, we'll explore how to avoid getting blocked using techniques such as fake user agents and browser headers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Why We Get Blocked When Web Scraping\n",
    "Websites use several methods to detect and block web scrapers, including:\n",
    "\n",
    "- **IP address blocking**: Sites detect multiple requests from the same IP and block it.\n",
    "- **Rate limiting**: Sending too many requests in a short period can trigger a block.\n",
    "- **User-agent detection**: Websites check the User-Agent header to detect if the requests come from a browser or an automated script.\n",
    "- **CAPTCHAs**: Sometimes, websites show CAPTCHAs to verify if the visitor is human.\n",
    "- **Other headers**: Websites inspect the HTTP request headers to identify unusual patterns, such as missing cookies, referrers, or other browser-specific headers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Explaining & Using User Agents to Bypass Getting Blocked\n",
    "The User-Agent string identifies the type of browser making the request. Websites use this information to customize the content for specific browsers. When scraping, if no user agent or a generic user agent is provided, websites may detect the request as coming from a bot and block it.\n",
    "\n",
    "- **Example of a User-Agent string**:<pre>\n",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3</pre>\n",
    "- **Adding User-Agents to Your Scrapy Spider**:\n",
    "You can set a random User-Agent in Scrapy using the USER_AGENT setting or by using a middleware to rotate User-Agents automatically.<br>\n",
    "Setting a Single User-Agent: In **settings.py**, add:<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotating User-Agents with ScrapeOps\n",
    "When scraping websites, one common technique to avoid getting blocked is to rotate user agents. A user agent is a string that identifies the browser and operating system making the request. By rotating user agents, you can make your requests appear as if they are coming from different devices or browsers.\n",
    "####  Step 1: Setting Up ScrapeOps\n",
    "To set up ScrapeOps for the Fake Headers API via their website, follow these steps:\n",
    "\n",
    "- **Sign Up / Log In**\n",
    "    - Visit the ScrapeOps website: Go to scrapeops.io.\n",
    "    - Sign up or log in: Create a new account or log in to your existing account.\n",
    "- **Access the Dashboard**\n",
    "    - Once logged in, navigate to your Dashboard.\n",
    "    - Here you can view your usage stats, API keys, and available tools.\n",
    "- **Generate an API Key**\n",
    "    - In the dashboard, find the section for API Keys.\n",
    "    - Generate a new API key if you don’t have one already.\n",
    "    - Copy this key for later use.\n",
    "- **Navigate to Fake Headers API**\n",
    "    - From the dashboard, locate the Fake Headers API section or navigate through the menu.\n",
    "    - Click on the Fake Headers API to access its details and documentation.\n",
    "- **Get API Endpoint and Parameters**\n",
    "    - Review the provided API endpoint for the **Fake User-Agents service**.\n",
    "    - Take note of any required parameters for requests (like user-agent, accept-language, etc.).\n",
    "\n",
    "#### Step 2: Writing a ScrapeOps User-Agents Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlencode\n",
    "from random import randint\n",
    "import requests\n",
    "\n",
    "class ScrapeOpsFakeUserAgentMiddleware:\n",
    "    \n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(crawler.settings)\n",
    "    def __init__(self, settings):\n",
    "        self.scrapeops_api_key = settings.get('SCRAPEOPS_API_KEY')\n",
    "        self.scrapeops_endpoint = settings.get('SCRAPEOPS_FAKE_USER_AGENT_ENDPOINT', 'https://headers.scrapeops.io/v1/user-agents') \n",
    "        self.scrapeops_fake_user_agents_active = settings.get('SCRAPEOPS_FAKE_USER_AGENT_ENABLED', False)\n",
    "        self.scrapeops_num_results = settings.get('SCRAPEOPS_NUM_RESULTS')\n",
    "        self.headers_list = []\n",
    "        self._get_user_agents_list()\n",
    "        self._scrapeops_fake_user_agents_enabled()\n",
    "        \n",
    "    def _get_user_agents_list(self):\n",
    "        payload = {'api_key': self.scrapeops_api_key}\n",
    "        if self.scrapeops_num_results is not None:\n",
    "            payload['num_results'] = self.scrapeops_num_results\n",
    "        response = requests.get(self.scrapeops_endpoint, params=urlencode(payload))\n",
    "        json_response = response.json()\n",
    "        self.user_agents_list = json_response.get('result', [])\n",
    "        \n",
    "    def _get_random_user_agent(self):\n",
    "        random_index = randint(0, len(self.user_agents_list) - 1)\n",
    "        return self.user_agents_list[random_index]\n",
    "    \n",
    "    def _scrapeops_fake_user_agents_enabled(self):\n",
    "        if self.scrapeops_api_key is None or self.scrapeops_api_key == '' or self.scrapeops_fake_user_agents_active == False:\n",
    "            self.scrapeops_fake_user_agents_active = False\n",
    "        else:\n",
    "            self.scrapeops_fake_user_agents_active = True\n",
    "            \n",
    "    def process_request(self, request, spider):        \n",
    "        random_user_agent = self._get_random_user_agent()\n",
    "        request.headers['User-Agent'] = random_user_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Modifying Settings.py\n",
    "- **Adding ScrapeOps Keys**<br>\n",
    "    - **SCRAPEOPS_API_KEY**: Your unique key to authenticate requests to ScrapeOps.\n",
    "    - **SCRAPEOPS_FAKE_USER_AGENT_ENDPOINT**: URL to fetch fake user agent strings from ScrapeOps.\n",
    "    - **SCRAPEOPS_FAKE_USER_AGENT_ENABLED**: Enables (True) or disables (False) the use of fake user agents in your Scrapy project.\n",
    "    - **SCRAPEOPS_NUM_RESULTS**: Specifies how many fake user agents to retrieve (e.g., 50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRAPEOPS_API_KEY = 'your-api-key'\n",
    "SCRAPEOPS_NUM_RESULTS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Enabling the Middleware Pipeline**<br>\n",
    "Activate the scrapeops user agents pipeline in settings.py by adding it to the DOWNLOADER_MIDDLEWARES section:<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOADER_MIDDLEWARES = {\n",
    "   #\"myproject.middlewares.ExampleDownloaderMiddleware\": 543,\n",
    "   \"myproject.middlewares.ScrapeOpsFakeUserAgentMiddleware\":400,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Explaining & Using Request Headers to Bypass Getting Blocked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides User-Agents, websites rely on other HTTP headers to detect bots. Some key headers that scrapers should set include:\n",
    "\n",
    "- **Referer**: This header specifies the URL of the page that led to the request.\n",
    "\n",
    "Example: Referer: https://example.com\n",
    "- **Accept-Language**: This tells the website what language the browser prefers.\n",
    "\n",
    "Example: Accept-Language: en-US,en;q=0.9\n",
    "- **Accept-Encoding**: Specifies the type of content the browser can handle (e.g., compressed content).\n",
    "\n",
    "Example: Accept-Encoding: gzip, deflate, br\n",
    "- **Cookie**: This header passes session data to the server. Websites may block requests if cookies are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotating Broswer-Header with ScrapeOps\n",
    "####  Step 1: Setting Up ScrapeOps\n",
    "Same steps as Fake User-Agents,but using **Fake User-Agents service** now.\n",
    "#### Step 2: Writing a ScrapeOps User-Agents Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlencode\n",
    "from random import randint\n",
    "import requests\n",
    "\n",
    "class ScrapeOpsFakeBrowserHeaderAgentMiddleware:\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(crawler.settings)\n",
    "\n",
    "    def __init__(self, settings):\n",
    "        self.scrapeops_api_key = settings.get('SCRAPEOPS_API_KEY')\n",
    "        self.scrapeops_endpoint = settings.get('SCRAPEOPS_FAKE_BROWSER_HEADER_ENDPOINT', 'https://headers.scrapeops.io/v1/browser-headers') \n",
    "        self.scrapeops_fake_browser_headers_active = settings.get('SCRAPEOPS_FAKE_BROWSER_HEADER_ENABLED', False)\n",
    "        self.scrapeops_num_results = settings.get('SCRAPEOPS_NUM_RESULTS')\n",
    "        self.headers_list = []\n",
    "        self._get_headers_list()\n",
    "        self._scrapeops_fake_browser_headers_enabled()\n",
    "\n",
    "    def _get_headers_list(self):\n",
    "        payload = {'api_key': self.scrapeops_api_key}\n",
    "        if self.scrapeops_num_results is not None:\n",
    "            payload['num_results'] = self.scrapeops_num_results\n",
    "        response = requests.get(self.scrapeops_endpoint, params=urlencode(payload))\n",
    "        json_response = response.json()\n",
    "        self.headers_list = json_response.get('result', [])\n",
    "\n",
    "    def _get_random_browser_header(self):\n",
    "        random_index = randint(0, len(self.headers_list) - 1)\n",
    "        return self.headers_list[random_index]\n",
    "\n",
    "    def _scrapeops_fake_browser_headers_enabled(self):\n",
    "        if self.scrapeops_api_key is None or self.scrapeops_api_key == '' or self.scrapeops_fake_browser_headers_active == False:\n",
    "            self.scrapeops_fake_browser_headers_active = False\n",
    "        else:\n",
    "            self.scrapeops_fake_browser_headers_active = True\n",
    "    \n",
    "    def process_request(self, request, spider):        \n",
    "        random_browser_header = self._get_random_browser_header()\n",
    "\n",
    "        request.headers['accept-language'] = random_browser_header['accept-language']\n",
    "        request.headers['sec-fetch-user'] = random_browser_header['sec-fetch-user'] \n",
    "        request.headers['sec-fetch-mode'] = random_browser_header['sec-fetch-mode'] \n",
    "        request.headers['sec-fetch-site'] = random_browser_header['sec-fetch-site'] \n",
    "        request.headers['sec-ch-ua-platform'] = random_browser_header['sec-ch-ua-platform'] \n",
    "        request.headers['sec-ch-ua-mobile'] = random_browser_header['sec-ch-ua-mobile'] \n",
    "        request.headers['sec-ch-ua'] = random_browser_header['sec-ch-ua'] \n",
    "        request.headers['accept'] = random_browser_header['accept'] \n",
    "        request.headers['user-agent'] = random_browser_header['user-agent'] \n",
    "        request.headers['upgrade-insecure-requests'] = random_browser_header.get('upgrade-insecure-requests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Modifying Settings.py\n",
    "- **Adding ScrapeOps Keys**<br>\n",
    "    - **SCRAPEOPS_API_KEY**: Your unique key to authenticate requests to ScrapeOps.\n",
    "    - **SCRAPEOPS_FAKE_BROWSER_HEADER_ENDPOINT**: URL to fetch fake browser header strings from ScrapeOps.\n",
    "    - **SCRAPEOPS_FAKE_BROWSER_HEADER_ENABLED**: Enables (True) or disables (False) the use of fake browser header in your Scrapy project.\n",
    "    - **SCRAPEOPS_NUM_RESULTS**: Specifies how many fake user agents to retrieve (e.g., 50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRAPEOPS_API_KEY = 'your-api-key'\n",
    "SCRAPEOPS_NUM_RESULTS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Rotating Proxies & Proxy APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 What Are Proxies and Why Do We Need Them?\n",
    "A proxy acts as an intermediary between your computer (the client) and the web server. When you use a proxy, your requests to a target website pass through a third-party server (the proxy) before reaching the destination.<br> This masks your IP address and can help you bypass rate limits, geo-restrictions, and anti-bot measures.\n",
    "\n",
    "#### Why use proxies?\n",
    "- **Avoid getting blocked**: Many websites block scrapers that send too many requests from a single IP address.\n",
    "- **Bypass geo-restrictions**: Some content is only available to users in certain locations.\n",
    "- **Distribute traffic**: By spreading your requests across multiple IP addresses, you reduce the chances of triggering rate-limiting rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 3 Most Popular Proxy Integration Methods\n",
    "There are three common ways to integrate proxies in Scrapy:\n",
    "\n",
    "- **Manual Proxy Lists**: You maintain a list of proxies and rotate through them.\n",
    "- **Rotating/Backconnect Proxies**: These services provide a pool of IPs that rotate automatically on every request.\n",
    "- **Proxy APIs**: Some proxy providers offer an API to fetch fresh proxies dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 How to Integrate and Rotate Proxy Lists in Scrapy\n",
    "\n",
    "### 1. Install scrapy-rotating-proxies\n",
    "First, install the scrapy-rotating-proxies library using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install scrapy-rotating-proxies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library helps rotate proxies automatically while scraping, making sure you don’t get blocked or blacklisted by websites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add Proxies to Scrapy Settings\n",
    "Once the library is installed, you need to configure Scrapy to use proxy rotation. Open your Scrapy project’s **settings.py** file, and add the following configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the rotating proxies middleware\n",
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'rotating_proxies.middlewares.RotatingProxyMiddleware': 610,\n",
    "    'rotating_proxies.middlewares.BanDetectionMiddleware': 620,\n",
    "}\n",
    "\n",
    "# List of proxies you want to use for rotation\n",
    "# These are just examples; you should replace them with your own proxies\n",
    "ROTATING_PROXY_LIST = [\n",
    "    'http://123.456.789.101:8000',\n",
    "    'http://111.222.333.444:8000',\n",
    "    'http://555.666.777.888:8000',\n",
    "]\n",
    "\n",
    "# Or use an external file with the proxies:\n",
    "# ROTATING_PROXY_LIST_PATH = '/path/to/proxy/list.txt'\n",
    "\n",
    "# Optional settings\n",
    "# Number of requests to send per proxy before rotating\n",
    "ROTATING_PROXY_PAGE_RETRY_TIMES = 5\n",
    "# Avoid rotation on every request (helps prevent overuse of proxies)\n",
    "ROTATING_PROXY_BACKOFF_BASE = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the proxy IPs and ports with actual proxies that you have. You can use free proxies (though they may be less reliable) or purchase dedicated proxy services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How to Rotate Proxy Lists\n",
    "The scrapy-rotating-proxies middleware will automatically handle rotating through the list of proxies you've provided. However, here are a few additional configurations that can help manage proxy rotation:\n",
    "\n",
    "- **ROTATING_PROXY_PAGE_RETRY_TIMES**: This setting controls how many times Scrapy should retry a request before rotating to a new proxy. Adjust this based on the reliability of your proxies.\n",
    "- **ROTATING_PROXY_BACKOFF_BASE**: This defines the base time (in seconds) that Scrapy should wait before retrying a request when using a proxy. This helps avoid overuse of a single proxy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adding Proxy List from an External File\n",
    "If you have a large proxy list, it might be better to store it in an external file. To do that, save your proxy list in a text file (e.g., proxy_list.txt) with one proxy per line:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "http://123.456.789.101:8000\n",
    "http://111.222.333.444:8000\n",
    "http://555.666.777.888:8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your settings.py, point to this file using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROTATING_PROXY_LIST_PATH = 'path/to/proxy_list.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy will load the proxies from the file and rotate through them during scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Rest Methods are paid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 How to Use Rotating/Backconnect Proxies\n",
    "Rotating/Backconnect proxies automatically rotate your IP address for each request without you having to manage a list. You usually get a single proxy endpoint from the provider, and the rotation happens on the server side.<br>\n",
    "\n",
    "For example, if your proxy provider gives you a backconnect proxy, you can configure it in settings.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY = 'http://backconnectproxy.example.com:8000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, update the middleware to use this proxy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProxyMiddleware:\n",
    "    def process_request(self, request, spider):\n",
    "        request.meta['proxy'] = spider.settings.get('PROXY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, every request will go through the backconnect proxy, and the provider will rotate the IP for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 How to Use Proxy APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Handling Form Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Understanding Form Handling in Scrapy\n",
    "In Scrapy, you can interact with forms by using the **FormRequest** class, which allows you to submit data through HTML forms. This request class can send **POST** or **GET** requests depending on the form's method attribute.\n",
    "\n",
    "- **GET** requests are typically used for search forms or when data is passed via the URL.\n",
    "- **POST** requests are typically used when submitting sensitive data (e.g., login credentials) via forms.\n",
    "\n",
    "You will need to find the form fields (input fields, buttons, etc.) and populate them with the appropriate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Basic Example of Handling Form Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.http import FormRequest\n",
    "\n",
    "class LoginSpider(scrapy.Spider):\n",
    "    name = 'login_spider'\n",
    "    start_urls = ['https://example.com/login']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Get CSRF token or any hidden fields if necessary\n",
    "        csrf_token = response.css('input[name=\"csrf_token\"]::attr(value)').get()\n",
    "        \n",
    "        # Create a dictionary with login form data\n",
    "        formdata = {\n",
    "            'username': 'myuser',\n",
    "            'password': 'mypassword',\n",
    "            'csrf_token': csrf_token  # If the website requires it\n",
    "        }\n",
    "\n",
    "        # Sending POST request to log in\n",
    "        yield FormRequest.from_response(\n",
    "            response,\n",
    "            formdata=formdata,\n",
    "            callback=self.after_login\n",
    "        )\n",
    "\n",
    "    def after_login(self, response):\n",
    "        # Check if login is successful\n",
    "        if 'Welcome' in response.text:\n",
    "            self.log('Login successful!')\n",
    "            # Continue scraping protected pages\n",
    "            yield scrapy.Request(url='https://example.com/protected', callback=self.parse_protected_page)\n",
    "        else:\n",
    "            self.log('Login failed')\n",
    "    \n",
    "    def parse_protected_page(self, response):\n",
    "        # Scrape data from a page after logging in\n",
    "        data = response.css('.data-class::text').get()\n",
    "        yield {'data': data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **FormRequest.from_response**: A method that automatically handles form data extraction and submits the form.\n",
    "- **callback**: After submitting the form, the after_form_submission method processes the resulting page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Handling CAPTCHAs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1  Understanding CAPTCHA Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several types of CAPTCHAs that websites use, each with different levels of complexity:\n",
    "\n",
    "- **Image-based CAPTCHAs**: These require users to identify distorted characters in an image.\n",
    "- **ReCAPTCHA**: Google's popular CAPTCHA that includes a checkbox (I'm not a robot) and may require image identification (e.g., selecting pictures of traffic lights).\n",
    "- **Invisible ReCAPTCHA**: A more user-friendly version of reCAPTCHA that uses behavioral analysis to determine if the user is a human.\n",
    "- **Mathematical CAPTCHAs**: Simple addition or subtraction problems that users need to solve.\n",
    "- **Puzzle CAPTCHAs**: Users may need to solve a puzzle, like dragging a slider or matching pieces.\n",
    "\n",
    "In this tutorial, we’ll focus on the first two types: image-based CAPTCHAs and ReCAPTCHA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Bypassing CAPTCHAs with Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy alone doesn't have the capability to solve CAPTCHAs. However, we can bypass CAPTCHAs using a few strategies:\n",
    "\n",
    "### Strategy 1: Using CAPTCHA Solving Services\n",
    "You can use third-party CAPTCHA-solving services like 2Captcha or AntiCaptcha. These services solve CAPTCHAs for you by either using human workers or machine learning to interpret CAPTCHA images.\n",
    "\n",
    "### Strategy 2: Using Selenium to Bypass CAPTCHAs\n",
    "If a website has a CAPTCHA challenge that Scrapy can't handle, Selenium can be used to control a web browser to complete the CAPTCHA manually. This is often necessary for complex CAPTCHAs like Google’s reCAPTCHA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using CAPTCHA Solving Services (e.g., 2Captcha, AntiCaptcha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can integrate third-party CAPTCHA-solving services with Scrapy to handle CAPTCHA challenges automatically.\n",
    "\n",
    "#### Example: Using 2Captcha API for Image-based CAPTCHA\n",
    "2Captcha is a popular service that allows you to send CAPTCHA images for solving. You need to send the CAPTCHA image, get the solution, and then use it to fill in the CAPTCHA form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install the 2Captcha Python Client\n",
    "You can install the 2Captcha client using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install python-2captcha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a Spider that Uses 2Captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "from twocaptcha import TwoCaptcha\n",
    "from scrapy.http import FormRequest\n",
    "\n",
    "class CaptchaSpider(scrapy.Spider):\n",
    "    name = 'captcha_spider'\n",
    "    start_urls = ['https://example.com/captcha-form']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract the captcha image URL\n",
    "        captcha_url = response.css('img#captcha::attr(src)').get()\n",
    "        \n",
    "        # Solve the captcha using 2Captcha\n",
    "        captcha_solution = self.solve_captcha(captcha_url)\n",
    "        \n",
    "        # Create the form data and submit it\n",
    "        formdata = {\n",
    "            'username': 'myuser',\n",
    "            'password': 'mypassword',\n",
    "            'captcha': captcha_solution  # Solved CAPTCHA\n",
    "        }\n",
    "\n",
    "        yield FormRequest.from_response(\n",
    "            response,\n",
    "            formdata=formdata,\n",
    "            callback=self.after_submission\n",
    "        )\n",
    "\n",
    "    def solve_captcha(self, captcha_url):\n",
    "        # Using 2Captcha API to solve the CAPTCHA\n",
    "        solver = TwoCaptcha('YOUR_2CAPTCHA_API_KEY')\n",
    "\n",
    "        # Send captcha image URL to 2Captcha for solving\n",
    "        try:\n",
    "            result = solver.normal(captcha_url)\n",
    "            return result['code']  # The CAPTCHA solution\n",
    "        except Exception as e:\n",
    "            self.log(f\"Error solving CAPTCHA: {e}\")\n",
    "            return None\n",
    "\n",
    "    def after_submission(self, response):\n",
    "        # Process the form submission result\n",
    "        if 'Welcome' in response.text:\n",
    "            self.log('Form submitted successfully!')\n",
    "        else:\n",
    "            self.log('Form submission failed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Captcha Image URL**: The URL of the CAPTCHA image is extracted from the page.\n",
    "- **2Captcha API**: The TwoCaptcha client is used to send the image to the 2Captcha API for solving.\n",
    "- **Form Submission**: Once the CAPTCHA is solved, the form is submitted with the solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Dealing with File Downloads (Images, PDFs, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 Understanding File Downloads in Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Scrapy, downloading files is handled by the **FilesPipeline**, which is a default pipeline for handling the downloading of files from URLs. Scrapy’s **FilesPipeline** allows you to download files, store them locally or remotely, and organize them based on the specific requirements of your project.\n",
    "\n",
    "- The **FilesPipeline** can be used for a variety of file types including:\n",
    "    - **Images**: Images from websites (e.g., product images, logos).\n",
    "    - **PDFs**: PDF documents such as eBooks, forms, reports, etc.\n",
    "    - **Audio and Video Files**: Other types of multimedia.\n",
    "    - **Any other file types**: Files with different extensions like **.txt**, **.csv**, **.xlsx**, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 Setting Up File Download Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.1 Enable Files Pipeline\n",
    "To start handling file downloads, you need to enable the FilesPipeline and define a FILES_STORE where the files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.py\n",
    "\n",
    "# Enable the FilesPipeline\n",
    "ITEM_PIPELINES = {\n",
    "    'scrapy.pipelines.files.FilesPipeline': 1,\n",
    "}\n",
    "\n",
    "# Define the directory where files will be stored\n",
    "FILES_STORE = '/path/to/your/local/directory'  # Local directory for storing downloaded files\n",
    "\n",
    "# Optional: Set the field where file URLs are stored\n",
    "FILES_URLS_FIELD = 'file_urls'  # Default is 'file_urls', you can change it as needed\n",
    "\n",
    "# Optional: Define the field to store downloaded file paths\n",
    "FILES_RESULT_FIELD = 'files'  # Default is 'files', you can change it as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **FILES_STORE** points to the directory where you want the downloaded files to be stored locally. You can also specify remote storage if required, such as Amazon S3 or FTP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.2 Customizing File Storage Path\n",
    "By default, Scrapy will save files with their original filenames in the specified directory. However, you can customize the file storage path by overriding the file_path method in your spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.pipelines.files import FilesPipeline\n",
    "\n",
    "class FileDownloadSpider(scrapy.Spider):\n",
    "    name = 'file_download_spider'\n",
    "    \n",
    "    def start_requests(self):\n",
    "        # Example: start with a URL containing a file to download\n",
    "        urls = ['https://example.com/somefile.pdf']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Use the 'file_urls' field to pass the file URL to the pipeline\n",
    "        yield {\n",
    "            'file_urls': [response.url]\n",
    "        }\n",
    "    \n",
    "    def file_path(self, request, response=None, info=None):\n",
    "        # Customize the file path, e.g., organize files by domain or file type\n",
    "        filename = request.url.split('/')[-1]\n",
    "        return f\"custom_folder/{filename}\"  # Save in custom_folder with the original filen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This spider downloads a PDF file and saves it in a subdirectory called **custom_folder**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 Downloading Multiple Files (e.g., Images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy can also handle the download of multiple files in one request. For example, if you're scraping product pages that contain multiple images, you can pass a list of image URLs to the **file_urls** field in your item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ImageDownloadSpider(scrapy.Spider):\n",
    "    name = 'image_download_spider'\n",
    "    \n",
    "    def start_requests(self):\n",
    "        urls = ['https://example.com/product-page']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Assuming images are contained within <img> tags with 'src' attributes\n",
    "        image_urls = response.css('img::attr(src)').getall()\n",
    "        \n",
    "        yield {\n",
    "            'file_urls': image_urls\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 Handling Files in the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once files are downloaded, Scrapy’s FilesPipeline stores them on the disk, but you may want to process them further. For example, you might want to:\n",
    "\n",
    "- **Rename** the files based on metadata or a custom scheme.\n",
    "- **Process** the files (e.g., resize images, convert PDFs to text).\n",
    "- **Move** files to different locations after downloading.\n",
    "\n",
    "You can implement this in the **FilesPipeline** by subclassing it and overriding the **file_path** or **item_completed** methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.4.1 Customizing File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.pipelines.files import FilesPipeline\n",
    "import os\n",
    "\n",
    "class CustomFilesPipeline(FilesPipeline):\n",
    "    \n",
    "    def file_path(self, request, response=None, info=None):\n",
    "        # Generate a custom file path for downloaded files\n",
    "        filename = request.url.split('/')[-1]\n",
    "        return os.path.join('downloads', filename)  # Save in 'downloads' folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This custom pipeline saves files in a **downloads** folder with their original filename."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.4.2 Post-processing Downloaded Files\n",
    "You can also override the **item_completed** method to handle additional tasks after the files have been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.pipelines.files import FilesPipeline\n",
    "\n",
    "class CustomFilesPipeline(FilesPipeline):\n",
    "    \n",
    "    def item_completed(self, results, item, info):\n",
    "        # Process the downloaded files\n",
    "        for result in results:\n",
    "            if result[0]:\n",
    "                # File successfully downloaded\n",
    "                file_path = result[1]['path']\n",
    "                # Perform custom post-processing on the file\n",
    "                self.process_file(file_path)\n",
    "        return item\n",
    "    \n",
    "    def process_file(self, file_path):\n",
    "        # Your custom logic for processing the file (e.g., image resizing)\n",
    "        print(f'Processing file: {file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, after the files are downloaded, they are processed with the **process_file** method, which you can customize to suit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- <a href=\"https://www.youtube.com/watch?v=mBoX_JCKZTE&t=10397s&pp=ygUVZGF0YSBzY2llbmNlIGJvb3RjYW1w\">Scrapy Course – Python Web Scraping for Beginners by freeCodeCamp.org</a>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
