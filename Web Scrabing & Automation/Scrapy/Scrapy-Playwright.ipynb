{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scrapy-Playwright** is an extension for Scrapy, designed to handle **JavaScript-rendered web pages**.<br>\n",
    "\n",
    "While Scrapy is a powerful framework for scraping static HTML pages, it struggles with websites that load content dynamically using JavaScript. Scrapy-Playwright solves this by integrating the Playwright browser automation tool, allowing Scrapy to render and interact with JavaScript-heavy pages before scraping them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features:\n",
    "- **JavaScript Rendering**: Uses Playwright to render dynamic content before scraping, making it possible to scrape data from pages that rely on client-side rendering (CSR).\n",
    "- **Multiple Browsers Support**: Works with major browsers like Chromium, Firefox, and WebKit, allowing you to scrape a wide variety of modern web applications.\n",
    "- **Seamless Integration**: Scrapy-Playwright is designed to work seamlessly with Scrapy, allowing you to leverage the familiar features of Scrapy, like item pipelines and crawling, while handling dynamic content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases:\n",
    "- Scraping data from websites that require JavaScript to display content (e.g., SPAs, dynamically-loaded lists).\n",
    "- Extracting data from modern e-commerce sites, news platforms, or dashboards that load content asynchronously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install **scrapy-playwright**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install scrapy-playwright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if your haven't already installed **Playwright** itself, you will need to install it using the following command in your command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Settings for the Scrapy Playwright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need to update our Scrapy projects settings to **activate scrapy-playwright** in the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.py\n",
    "\n",
    "DOWNLOAD_HANDLERS = {\n",
    "    \"http\": \"scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler\",\n",
    "    \"https\": \"scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler\",\n",
    "}\n",
    "\n",
    "TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ScrapyPlaywrightDownloadHandler** class inherits from Scrapy's default **http/https** handler. So unless you explicitly activate scrapy-playwright in your Scrapy Request, those requests will be processed by the regular Scrapy download handler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Scrapy Playwright In Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's integrate **scrapy-playwright** into a Scrapy spider so all our requests will be JS rendered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To route our requests through **scrapy-playwright** we just need to enable it in the Request meta dictionary by setting **meta={'playwright': True}**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spiders/quotes.py\n",
    "\n",
    "import scrapy\n",
    "from quotes_js_scraper.items import QuoteItem\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "\tname = 'quotes'\n",
    "\n",
    "\tdef start_requests(self):\n",
    "\t\turl = \"https://quotes.toscrape.com/js/\"\n",
    "\t\tyield scrapy.Request(url, meta={'playwright': True})\n",
    "\n",
    "\tdef parse(self, response):\n",
    "\t\tfor quote in response.css('div.quote'):\n",
    "\t\t\tquote_item = QuoteItem()\n",
    "\t\t\tquote_item['text'] = quote.css('span.text::text').get()\n",
    "\t\t\tquote_item['author'] = quote.css('small.author::text').get()\n",
    "\t\t\tquote_item['tags'] = quote.css('div.tags a.tag::text').getall()\n",
    "\t\t\tyield quote_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **response** will now contain the rendered page as **seen by the browser**.<br>\n",
    "However, sometimes **Playwright** will have ended the rendering before the entire page has been rendered which we can solve using Playwright **PageMethods**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting With The Page Using Playwright PageMethods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interaction with the page using scrapy-playwright we will need to use the **PageMethod** class.\n",
    "\n",
    "- **PageMethod's** allow us to do alot of different things on the page, including:\n",
    "    - Wait for elements to load before returning response\n",
    "    - Scrolling the page\n",
    "    - Clicking on page elements\n",
    "    - Taking a screenshot of the page\n",
    "    - Creating PDFs of the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to use the **PageMethod** functionality in your spider you will need to set **playwright_include_page equal to True** so we can access the Playwright Page object and also define any callbacks (i.e. def parse) as a coroutine function (**async def**) in order to await the provided Page object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spiders/quotes.py\n",
    "\n",
    "import scrapy\n",
    "from quotes_js_scraper.items import QuoteItem\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "\tname = 'quotes'\n",
    "\n",
    "\tdef start_requests(self):\n",
    "\t\turl = 'https://quotes.toscrape.com/js/'\n",
    "\t\tyield scrapy.Request(url, meta=dict(\n",
    "\t\t\tplaywright = True,\n",
    "\t\t\tplaywright_include_page = True, \n",
    "      \t\terrback=self.errback,\n",
    "\t\t))\n",
    "\n",
    "\tasync def parse(self, response):\n",
    "\t\tpage = response.meta[\"playwright_page\"]\n",
    "\t\tawait page.close()\n",
    "\n",
    "\t\tfor quote in response.css('div.quote'):\n",
    "\t\t\tquote_item = QuoteItem()\n",
    "\t\t\tquote_item['text'] = quote.css('span.text::text').get()\n",
    "\t\t\tquote_item['author'] = quote.css('small.author::text').get()\n",
    "\t\t\tquote_item['tags'] = quote.css('div.tags a.tag::text').getall()\n",
    "\t\t\tyield quote_item\n",
    "  \n",
    "\tasync def errback(self, failure):\n",
    "\t\tpage = failure.request.meta[\"playwright_page\"]\n",
    "\t\tawait page.close()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Waiting For Page Elements\n",
    "To wait for a specific page element before stopping the javascript rendering and returning a response to our scraper we just need to add a **PageMethod** to the **playwright_page_methods** key in out Playwrright settings and define a **wait_for_selector**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we run the spider scrapy-playwright will render the page until a **div** with a class **quote** appears on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spiders/quotes.py\n",
    "\n",
    "import scrapy\n",
    "from quotes_js_scraper.items import QuoteItem\n",
    "from scrapy_playwright.page import PageMethod\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "\tname = 'quotes'\n",
    "\n",
    "\tdef start_requests(self):\n",
    "\t\turl = \"https://quotes.toscrape.com/js/\"\n",
    "\t\tyield scrapy.Request(url, meta=dict(\n",
    "\t\t\t\tplaywright = True,\n",
    "\t\t\t\tplaywright_include_page = True, \n",
    "\t\t\t\tplaywright_page_methods =[PageMethod('wait_for_selector', 'div.quote')],\n",
    "        errback=self.errback,\n",
    "\t\t\t))\n",
    "\n",
    "\tasync def parse(self, response):\n",
    "    \tpage = response.meta[\"playwright_page\"]\n",
    "    \tawait page.close()\n",
    "\n",
    "\t\tfor quote in response.css('div.quote'):\n",
    "\t\t\tquote_item = QuoteItem()\n",
    "\t\t\tquote_item['text'] = quote.css('span.text::text').get()\n",
    "\t\t\tquote_item['author'] = quote.css('small.author::text').get()\n",
    "\t\t\tquote_item['tags'] = quote.css('div.tags a.tag::text').getall()\n",
    "\t\t\tyield quote_item\n",
    "  \n",
    "\tasync def errback(self, failure):\n",
    "\t\tpage = failure.request.meta[\"playwright_page\"]\n",
    "\t\tawait page.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scraping Multiple Pages\n",
    "Usually we need to scrape multiple pages on a javascript rendered website. We will do this by checking if there is a next page link present on the page and then requesting that page with the url that we scrape from the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spiders/quotes.py\n",
    "\n",
    "import scrapy\n",
    "from quotes_js_scraper.items import QuoteItem\n",
    "from scrapy_playwright.page import PageMethod\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "\n",
    "    def start_requests(self):\n",
    "        url = \"https://quotes.toscrape.com/js/\"\n",
    "        yield scrapy.Request(url, meta=dict(\n",
    "                playwright = True,\n",
    "                playwright_include_page = True, \n",
    "                playwright_page_methods =[\n",
    "                    PageMethod('wait_for_selector', 'div.quote'),\n",
    "                ],\n",
    "        errback=self.errback,\n",
    "            ))\n",
    "\n",
    "    async def parse(self, response):\n",
    "        page = response.meta[\"playwright_page\"]\n",
    "        await page.close()\n",
    "\n",
    "        for quote in response.css('div.quote'):\n",
    "            quote_item = QuoteItem()\n",
    "            quote_item['text'] = quote.css('span.text::text').get()\n",
    "            quote_item['author'] = quote.css('small.author::text').get()\n",
    "            quote_item['tags'] = quote.css('div.tags a.tag::text').getall()\n",
    "            yield quote_item\n",
    "\n",
    "        next_page = response.css('.next>a ::attr(href)').get()\n",
    "\n",
    "        if next_page is not None:\n",
    "            next_page_url = 'http://quotes.toscrape.com' + next_page\n",
    "            yield scrapy.Request(next_page_url, meta=dict(\n",
    "                playwright = True,\n",
    "                playwright_include_page = True, \n",
    "                playwright_page_methods =[\n",
    "                    PageMethod('wait_for_selector', 'div.quote'),\n",
    "                ],\n",
    "                errback=self.errback,\n",
    "            ))\n",
    "  \n",
    "    async def errback(self, failure):\n",
    "        page = failure.request.meta[\"playwright_page\"]\n",
    "        await page.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scroll Down Infinite Scroll Pages\n",
    "We can also configure scrapy-playwright to scroll down a page when a website uses an infinite scroll to load in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, Playwright will wait for **div.quote** to appear, before scrolling down the page until it **reachs the 10th quote**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spiders/quotes.py\n",
    "\n",
    "import scrapy\n",
    "from quotes_js_scraper.items import QuoteItem\n",
    "from scrapy_playwright.page import PageMethod\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "\tname = 'quotes'\n",
    "\n",
    "\tdef start_requests(self):\n",
    "\t\turl = \"https://quotes.toscrape.com/scroll\"\n",
    "\t\tyield scrapy.Request(url, meta=dict(\n",
    "\t\t\t\tplaywright = True,\n",
    "\t\t\t\tplaywright_include_page = True, \n",
    "\t\t\t\tplaywright_page_methods =[\n",
    "          PageMethod(\"wait_for_selector\", \"div.quote\"),\n",
    "          PageMethod(\"evaluate\", \"window.scrollBy(0, document.body.scrollHeight)\"),\n",
    "          PageMethod(\"wait_for_selector\", \"div.quote:nth-child(11)\"),  # 10 per page\n",
    "          ],\n",
    "        errback=self.errback,\n",
    "\t\t\t))\n",
    "\n",
    "\tasync def parse(self, response):\n",
    "    \tpage = response.meta[\"playwright_page\"]\n",
    "    \tawait page.close()\n",
    "\n",
    "\t\tfor quote in response.css('div.quote'):\n",
    "\t\t\tquote_item = QuoteItem()\n",
    "\t\t\tquote_item['text'] = quote.css('span.text::text').get()\n",
    "\t\t\tquote_item['author'] = quote.css('small.author::text').get()\n",
    "\t\t\tquote_item['tags'] = quote.css('div.tags a.tag::text').getall()\n",
    "\t\t\tyield quote_item\n",
    "  \n",
    "\tasync def errback(self, failure):\n",
    "\t\tpage = failure.request.meta[\"playwright_page\"]\n",
    "\t\tawait page.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Take Screenshot Of Page\n",
    "Taking screenshots of the page are simple too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we wait for Playwright to see the selector **div.quote** then it takes a screenshot of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spiders/quotes.py\n",
    "\n",
    "import scrapy\n",
    "from quotes_js_scraper.items import QuoteItem\n",
    "from scrapy_playwright.page import PageMethod\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "\tname = 'quotes'\n",
    "\n",
    "\tdef start_requests(self):\n",
    "\t\turl = \"https://quotes.toscrape.com/js/\"\n",
    "\t\tyield scrapy.Request(url, meta=dict(\n",
    "\t\t\t\tplaywright = True,\n",
    "\t\t\t\tplaywright_include_page = True, \n",
    "\t\t\t\tplaywright_page_methods =[\n",
    "          PageMethod(\"wait_for_selector\", \"div.quote\"),\n",
    "          ]\n",
    "\t\t\t))\n",
    "\n",
    "\tasync def parse(self, response):\n",
    "    \tpage = response.meta[\"playwright_page\"]\n",
    "    \tscreenshot = await page.screenshot(path=\"example.png\", full_page=True)\n",
    "    \t# screenshot contains the image's bytes\n",
    "    \tawait page.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "- <a href=\"https://www.youtube.com/watch?v=EijzO7n2-dg&ab_channel=ScrapeOps\">Scrapy-Playwright: How To Scrape Dynamic JS Websites(2022) by ScrapeOps</a>\n",
    "- <a href=\"https://scrapeops.io/python-scrapy-playbook/scrapy-playwright/\">Scrapy Playwright Guide: Render & Scrape JS Heavy Websites\n",
    "</a>\n",
    "- <a href=\"https://github.com/scrapy-plugins/scrapy-playwright\">scrapy-plugins/scrapy-playwright</a>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
